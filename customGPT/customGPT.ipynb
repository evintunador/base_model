{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# imports for the debugging/demonstration setup\n",
    "import functools\n",
    "import inspect\n",
    "\n",
    "# imports for the tokenizer\n",
    "from tiny_stories_tokenizer import *\n",
    "\n",
    "# Imports used for the config\n",
    "import dataclasses \n",
    "from typing import Optional\n",
    "\n",
    "# for the dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "\n",
    "# used in the training loop\n",
    "import time\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e4fed6-f74d-45fb-981f-f84e7ebf22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used throughout for debugging/demonstration purposes\n",
    "# using this is way cleaner than cluttering up our code with print statements\n",
    "def log_io(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        # Check if logging is enabled globally and for the specific function\n",
    "        if not self.logging_enabled or func.__name__ in self.disabled_logging_functions:\n",
    "            return func(self, *args, **kwargs)\n",
    "        #if not self.logging_enabled:\n",
    "            #return func(self, *args, **kwargs)\n",
    "\n",
    "        def log_item(item, name, level=0, is_root=False):\n",
    "            indent = \"    \" * level\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"{indent}Tensor '{name}' shape: {item.shape}\")\n",
    "            elif isinstance(item, tuple):\n",
    "                if is_root and level == 0:\n",
    "                    # Root level tuple, don't print it as a tuple unless it's a \"true\" tuple\n",
    "                    for idx, sub_item in enumerate(item):\n",
    "                        log_item(sub_item, f\"{name}[{idx}]\", level)\n",
    "                else:\n",
    "                    print(f\"{indent}Tuple '{name}':\")\n",
    "                    for idx, sub_item in enumerate(item):\n",
    "                        log_item(sub_item, f\"{name}[{idx}]\", level + 1)\n",
    "            elif isinstance(item, int):\n",
    "                print(f\"{indent}Integer '{name}': Value={item}\")\n",
    "            elif isinstance(item, float):\n",
    "                print(f\"{indent}Float '{name}': Value={item}\")\n",
    "            else:\n",
    "                print(f\"{indent}Other-type '{name}': Type={type(item).__name__}, Value={item}\")\n",
    "\n",
    "        print(f\"\\n{'='*10}Entering {self.__class__.__name__}.{func.__name__}{'='*10}\")\n",
    "        print(\"Inputs:\")\n",
    "        arg_names = inspect.getfullargspec(func).args[1:]  # Excluding 'self'\n",
    "        arg_values = args + tuple(kwargs.values())\n",
    "        for name, value in zip(arg_names, arg_values):\n",
    "            log_item(value, name)\n",
    "\n",
    "        result = func(self, *args, **kwargs)\n",
    "        print(\"\\nOutputs:\")\n",
    "        if isinstance(result, tuple):\n",
    "            log_item(result, \"output\", is_root=True)\n",
    "        else:\n",
    "            log_item(result, \"output\")\n",
    "\n",
    "        print(f\"{'='*10}Exiting {self.__class__.__name__}.{func.__name__}{'='*10}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "class LoggingModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logging_enabled = False\n",
    "        self.disabled_logging_functions = set()\n",
    "\n",
    "    def enable_logging(self):\n",
    "        self.logging_enabled = True\n",
    "\n",
    "    def disable_logging(self):\n",
    "        self.logging_enabled = False\n",
    "\n",
    "    def disable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.add(func_name)\n",
    "\n",
    "    def enable_function_logging(self, func_name):\n",
    "        self.disabled_logging_functions.discard(func_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd373bf-d326-4939-be18-819bae892805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(size = 128) # size options are 128, 256, 512 and 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1ef54bc-7b2c-49f7-ae86-86d9c05b9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for next-concept predictor\n",
    "    \"\"\"\n",
    "    ### boring hyperparameters ###\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    max_seq_len: int = 128\n",
    "    num_hidden_layers: int = 8\n",
    "    num_q_heads: int = 4\n",
    "    num_kv_heads: int = 1 \n",
    "    assert num_q_heads % num_kv_heads == 0\n",
    "    embed_dim: int = 128 \n",
    "    mlp_multiplier: int = 4\n",
    "    head_dim: int = 32\n",
    "    theta = 100.0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    batch_size = 32\n",
    "    dropout_rate = 0.1 # this has only been implemented into the MLP so far\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6384a84e-ca6c-4479-ab8c-4daf3d724b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoPE(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "    \n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "181d512a-8fa4-4e54-a511-19e07a9fd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQA(LoggingModule):\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_q_heads = config.num_q_heads\n",
    "        self.num_kv_heads = config.num_kv_heads\n",
    "        assert self.num_q_heads % self.num_kv_heads == 0\n",
    "        self.num_queries_per_kv = self.num_q_heads // self.num_kv_heads\n",
    "\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.head_dim = config.head_dim\n",
    "        self.theta = config.theta\n",
    "\n",
    "        # Calculates the total size for all projections.\n",
    "        self.q_size = self.num_q_heads * self.head_dim\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "\n",
    "        # Defines the scaling factor for the attention scores.\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.embed_dim, (self.num_q_heads + 2 * self.num_kv_heads) * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_q_heads * self.head_dim, self.embed_dim, bias=False)\n",
    "    \n",
    "        # for our attention mask we'll create a boolean mask that'll later be turned into large negative values\n",
    "        self.mask = torch.tril(torch.ones((config.max_seq_len, config.max_seq_len), dtype=torch.uint8)\n",
    "                              ).view(1, 1, config.max_seq_len, config.max_seq_len).to(dtype=torch.bool)\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len, _ = x.shape\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        # Splits the combined QKV tensor into separate tensors for queries (xq), keys (xk), and values (xv) based on their respective sizes.\n",
    "        xq, xk, xv = qkv.split([self.q_size, self.kv_size, self.kv_size],dim=-1)\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.num_q_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = RoPE(xq, self.head_dim, self.theta)\n",
    "        xk = RoPE(xk, self.head_dim, self.theta)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.num_kv_heads != self.num_q_heads:\n",
    "            xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2) # [batch_size, input_len, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        q = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        k = xk.transpose(1, 2)\n",
    "        v = xv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        logits = torch.matmul(q, k.transpose(2, 3)) * self.scaling # [batch_size, n_local_heads, input_len, input_len]\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention logits\n",
    "        logits = torch.where(self.mask[..., :input_len, :input_len].expand_as(logits), \n",
    "                             logits, \n",
    "                             torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "\n",
    "        # Applies softmax to the logits to obtain attention probabilities\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        output = torch.matmul(scores, v) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1) # [batch_size, input_len, hidden_dim]\n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3111446c-52ff-4c36-b441-e6724d6ad461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(LoggingModule):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 mlp_multiplier: int,\n",
    "                 output_dim: int,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mlp_multiplier = mlp_multiplier\n",
    "        self.hidden_size = embed_dim * mlp_multiplier\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # the gate, up and down projections\n",
    "        self.gate_proj = nn.Linear(embed_dim, self.hidden_size)\n",
    "        self.up_proj = nn.Linear(embed_dim, self.hidden_size)\n",
    "        self.down_proj = nn.Linear(self.hidden_size, output_dim)\n",
    "        \n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor, training: bool = False ) -> torch.Tensor:\n",
    "        output = self.down_proj(F.gelu(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return F.dropout(output, p=self.dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "937ec822-e4fb-4b76-9ada-37dd1f94acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(LoggingModule):\n",
    "    def __init__(self, num_features, eps=1e-5, use_scale=True):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(num_features)) if use_scale else None\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the mean squared value for each feature\n",
    "        mean_squared = inputs.pow(2).mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # Normalize inputs\n",
    "        normed_inputs = inputs * torch.rsqrt(mean_squared + self.eps)\n",
    "\n",
    "        # Apply scale if it exists\n",
    "        if self.scale is not None:\n",
    "            normed_inputs = normed_inputs * self.scale\n",
    "\n",
    "        return normed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbd1fcf1-a1dd-4e6c-8d78-e267398304f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(LoggingModule):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mqa = MQA(config)\n",
    "        self.mlp = MLP(config.embed_dim, config.mlp_multiplier, config.embed_dim, config.dropout_rate)\n",
    "        \n",
    "        self.pre_mqa_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "        self.post_mqa_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "        self.pre_mlp_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "        self.post_mlp_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "\n",
    "    @log_io\n",
    "    def forward(self, x: torch.Tensor ) -> torch.Tensor:\n",
    "        x = x + self.post_mqa_norm(self.mqa(self.pre_mqa_norm(x)))\n",
    "        x = x + self.post_mlp_norm(self.mlp(self.pre_mlp_norm(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dea80b7-2d2e-4e0d-9616-6acfee9b4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customGPT(LoggingModule):\n",
    "\n",
    "    def __init__(self,\n",
    "        config: Config, # the hyperparameters\n",
    "        tokenizer: tokenizer, # the tokenizer. we don't always store the tokenizer inside of the model, but it doesn't matter here\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # the attention heads need to cleanly divide up the embed_dim of the model so that we can split it all apart & combine back together\n",
    "        assert config.embed_dim % config.num_q_heads == 0\n",
    "\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.head_dim = config.head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(self.vocab_size+1, config.embed_dim)\n",
    "        self.scaling = config.embed_dim ** 0.5 # for normalizing the first embedding\n",
    "        \n",
    "        # Initialize a sequence of DecoderLayer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_hidden_layers))\n",
    "\n",
    "        # Initialize a normalization layer to be applied after the last decoder layer, stabilizing the output\n",
    "        self.final_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    @log_io\n",
    "    def forward(\n",
    "        self,\n",
    "        input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len) list of integer token ids\n",
    "        target_token_ids: torch.Tensor = None, # a shape (batch_size, input_seq_len) list of token ids to train on\n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "        # turn the input tokens into the first resudial state using the embedding matrix\n",
    "        x = self.embedder(input_token_ids) * self.scaling # (batch_size, input_len) & (vocab_size, embed_dim) -> (batch_size, input_len, embed_dim)\n",
    "\n",
    "        # Iteratively process the input through each Layer\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply normalization to the output of the final decoder layer\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # grabbing the weights of the embedding matrix shape (vocab_size, hidden_dim) for use as the output layer\n",
    "        embedder_weight = self.embedder.weight\n",
    "\n",
    "        # the embedding matrix is also used as the output layer\n",
    "        logits = torch.matmul(x, embedder_weight.t()) # (batch_size, input_len, embed_dim) @ (embed_dim, vocab_size) -> (batch_size, input_len, vocab_size)\n",
    "        \n",
    "        if target_token_ids is None: # if we're not training, then we don't need to calculate loss\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are training\n",
    "            batch_size, input_len, vocab_size = logits.shape\n",
    "            # then we reshape our logits & targets before calculating cross-entropy loss\n",
    "            loss = self.criterion(logits.view(batch_size*input_len, vocab_size), \n",
    "                                  target_token_ids.reshape(batch_size*input_len))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    @log_io\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling \n",
    "        \"\"\"\n",
    "        # Select the last element for each sequence.\n",
    "        logits = logits[:,-1,:] # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        logits.div_(temperature) # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k. both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "\n",
    "        ### calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "\n",
    "        ### calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "        \n",
    "    @log_io\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_len: int = None, \n",
    "        temperature: float = 1.0, # defaulting to 1.0 means we essentially don't use temperature\n",
    "        top_p: float = 1.0, # defaulting to 1.0 means we essentially don't use top-p\n",
    "        top_k: int = config.vocab_size, # setting top_k = vocab_size means we're effectively not using top_k at all\n",
    "    ) -> str: \n",
    "        \"\"\" Wrapper around sampler() that deals with manipulation of the sequence \"\"\"\n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        \n",
    "        if output_len is None:\n",
    "            output_len = config.max_seq_len - len(tokens)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        assert len(tokens) + output_len <= self.config.max_seq_len\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len])\n",
    "            \n",
    "            next_token = self.Sampler(\n",
    "                logits = logits,\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                top_k = top_k\n",
    "            )\n",
    "            if next_token == config.vocab_size: break\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a1f17-5a2e-4bbe-bc99-e9412d2b81de",
   "metadata": {},
   "source": [
    "# Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd47c947-b56b-43db-9bc3-75c5af0253c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 <class 'list'> \n",
      "\n",
      " 660 Once upon a time, there were two friends who were walking in the park. They came to a bakery, which smelled yummy. One of the friends asked the other, \"what do you want to eat?\".\n",
      "The other friend replied, \"I want an original pie!\" The baker took out a pie and shook it. The friends could feel the pie shaking in the baker's hands. They looked so excited!\n",
      "But when the baker gave the pie to the friends, it was not an original pie! It was a sour one! The friends were sad and grumpy. All their hopes of eating an original pie had been shaken away.\n",
      "The friends sadly left the bakery, with no pie in their hands. They never came back to the bakery again. The end. \n",
      "\n",
      " 481 [36, 61, 50, 52, 1, 68, 63, 95, 1, 48, 1, 67, 96, 52, 79, 77, 83, 1, 70, 89, 52, 1, 67, 70, 62, 1, 53, 121, 92, 51, 66, 1, 70, 55, 62, 1, 70, 89, 52, 1, 85, 59, 58, 102, 1, 82, 1, 77, 1, 63, 97, 58, 75, 110, 1, 50, 108, 52, 1, 80, 1, 48, 1, 49, 48, 58, 89, 72, 79, 70, 55, 56, 50, 55, 1, 66, 60, 52, 101, 78, 1, 72, 68, 60, 60, 72, 75, 36, 109, 1, 62, 53, 1, 77, 1, 53, 121, 92, 51, 66, 1, 48, 66, 58, 78, 1, 77, 1, 103, 126, 79, 3, 70, 87, 67, 1, 51, 62, 1, 72, 84, 1, 85, 61, 67, 1, 80, 1, 52, 112, 21, 3, 107, 88, 1, 103, 126, 1, 53, 121, 92, 51, 1, 83, 111, 56, 78, 79, 3, 30, 1, 85, 61, 67, 1, 76, 1, 104, 115, 82, 48, 59, 1, 63, 56, 52, 2, 3, 1, 88, 1, 49, 48, 58, 89, 1, 80, 62, 58, 1, 84, 67, 1, 48, 1, 63, 56, 52, 1, 81, 1, 66, 55, 62, 62, 58, 1, 86, 75, 88, 1, 53, 121, 92, 51, 66, 1, 50, 84, 59, 51, 1, 53, 52, 52, 59, 1, 77, 1, 63, 56, 52, 1, 66, 87, 58, 102, 1, 82, 1, 77, 1, 49, 48, 58, 89, 5, 66, 1, 87, 61, 51, 66, 75, 110, 1, 114, 62, 58, 78, 1, 66, 62, 1, 52, 71, 50, 86, 78, 2, 0, 23, 124, 1, 70, 74, 61, 1, 77, 1, 49, 48, 58, 89, 1, 54, 48, 69, 52, 1, 77, 1, 63, 56, 52, 1, 80, 1, 77, 1, 53, 121, 92, 51, 66, 79, 86, 1, 90, 1, 61, 103, 1, 76, 1, 104, 115, 82, 48, 59, 1, 63, 56, 52, 2, 1, 30, 67, 1, 90, 1, 48, 1, 66, 84, 65, 1, 95, 52, 2, 1, 88, 1, 53, 121, 92, 51, 66, 1, 70, 89, 52, 1, 98, 51, 1, 81, 1, 54, 65, 68, 60, 63, 72, 75, 22, 101, 1, 77, 106, 1, 55, 62, 63, 52, 66, 1, 62, 53, 1, 52, 112, 102, 1, 76, 1, 104, 115, 82, 48, 59, 1, 63, 56, 52, 1, 87, 51, 1, 118, 92, 1, 66, 87, 58, 92, 1, 48, 85, 72, 107, 88, 1, 53, 121, 92, 51, 66, 1, 98, 51, 59, 72, 1, 100, 53, 67, 1, 77, 1, 49, 48, 58, 89, 72, 79, 70, 86, 55, 1, 61, 62, 1, 63, 56, 52, 1, 82, 1, 77, 106, 1, 87, 61, 51, 66, 75, 110, 1, 109, 117, 1, 50, 108, 52, 1, 49, 48, 50, 58, 1, 80, 1, 77, 1, 49, 48, 58, 89, 72, 1, 48, 54, 48, 82, 75, 88, 1, 92, 51, 8]\n"
     ]
    }
   ],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        # Store the file path\n",
    "        self.file_path = file_path\n",
    "        \n",
    "        # Open the pickle file and load the list of indices (or any structure that allows random access)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            self.data = pickle.load(file)\n",
    "        \n",
    "        # If your data is not a list but a single string, you need to preprocess it here to split it into samples\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of items in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the story at the given index\n",
    "        story = self.data[idx]\n",
    "        \n",
    "        # Here you would typically convert the story to a tensor, for example, by tokenizing it\n",
    "        # For simplicity, let's assume the story is already in the desired format\n",
    "        return story\n",
    "\n",
    "#config.batch_size = 32\n",
    "\n",
    "train_dataset = TinyStoriesDataset('tiny_stories_train_data.pkl')\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TinyStoriesDataset('tiny_stories_val_data.pkl')\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "train_loader_iter = iter(train_loader)\n",
    "val_loader_iter = iter(val_loader)\n",
    "next_batch = next(train_loader_iter)  # Fetch the next batch\n",
    "# Now, you can process `next_batch` as needed\n",
    "print(len(next_batch), type(next_batch), '\\n\\n', len(next_batch[0]), next_batch[0], '\\n\\n', len(tokenizer.encode(next_batch[0])), tokenizer.encode(next_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc248f3-4a49-4f49-967f-577f06df16a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128]),\n",
       " tensor([[ 36, 109,   1,  ...,  48,   1,  49],\n",
       "         [ 36,  61,  50,  ...,  90,   1,  54],\n",
       "         [ 23,  92,   1,  ...,  51,   1,  50],\n",
       "         ...,\n",
       "         [123,   1,  81,  ...,  48,  69,  52],\n",
       "         [ 36,  61,  50,  ...,   1, 126,   1],\n",
       "         [ 36,  61,  50,  ...,  63,  56,  73]]),\n",
       " torch.Size([32, 128]),\n",
       " tensor([[109,   1, 122,  ...,   1,  49, 115],\n",
       "         [ 61,  50,  52,  ...,   1,  54,  95],\n",
       "         [ 92,   1,  81,  ...,   1,  50,  48],\n",
       "         ...,\n",
       "         [  1,  81,   1,  ...,  69,  52,   1],\n",
       "         [ 61,  50,  52,  ..., 126,   1,  97],\n",
       "         [ 61,  50,  52,  ...,  56,  73,  73]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_batch(batch, tokenizer, config):\n",
    "    \"\"\"\n",
    "    Process a batch of strings for transformer training.\n",
    "\n",
    "    Args:\n",
    "    batch (list of str): The batch of strings to process.\n",
    "    tokenizer: The tokenizer to use for encoding the strings.\n",
    "    config: Configuration object with max_seq_len and vocab_size attributes.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, torch.Tensor: The input and target tensors for the transformer.\n",
    "    \"\"\"\n",
    "    tokenized_batch = [tokenizer.encode(string)[:config.max_seq_len+1] for string in batch]\n",
    "    max_length = min(max(len(tokens) for tokens in tokenized_batch), config.max_seq_len)\n",
    "\n",
    "    # Pad the sequences\n",
    "    padded_batch = [tokens + [config.vocab_size] * (max_length+1 - len(tokens)) for tokens in tokenized_batch]\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    tensor_batch = torch.tensor(padded_batch, dtype=torch.long)\n",
    "\n",
    "    # Split into input and target tensors\n",
    "    input_tensor = tensor_batch[:, :-1]  # Exclude the last token for input\n",
    "    target_tensor = tensor_batch[:, 1:]  # Exclude the first token for target\n",
    "\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have a tokenizer and config object defined, you would call this function like this:\n",
    "x,y = process_batch(next(train_loader_iter), tokenizer, config)\n",
    "x.shape,x, y.shape,y \n",
    "# you may notice the beginnings often look the same, but that's only because many stories begin with something like \"Once upon a time,...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters = 5): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = process_batch(next(val_loader_iter), tokenizer, config)\n",
    "            logits, loss = model(X.to(config.device), Y.to(config.device))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930.496 K parameters\n",
      "customGPT(\n",
      "  (embedder): Embedding(129, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x Layer(\n",
      "      (mqa): MQA(\n",
      "        (qkv_proj): Linear(in_features=128, out_features=192, bias=False)\n",
      "        (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (gate_proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (up_proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (down_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (pre_mqa_norm): RMSNorm()\n",
      "      (post_mqa_norm): RMSNorm()\n",
      "      (pre_mlp_norm): RMSNorm()\n",
      "      (post_mlp_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (final_norm): RMSNorm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = customGPT(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 2\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 112.0365, val loss 111.9393, time elapsed: 1.18 seconds\n",
      "step 1: train loss 106.3015, val loss 106.3094, time elapsed: 3.80 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = process_batch(next(train_loader_iter), tokenizer, config)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb.to(config.device), yb.to(config.device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'models/{model.__class__.__name__}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}'\n",
    "torch.save(model.state_dict(), f'{name}.pth')\n",
    "\n",
    "# Convert the dataclass object to a dictionary\n",
    "config_dict = asdict(config)\n",
    "\n",
    "# Serialize the dictionary to a JSON file\n",
    "with open(f'{name}.json', 'w') as f:\n",
    "    json.dump(config_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5242a7-6035-4f53-9d26-0690aea809d9",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d5b32a-8fc1-4fa2-a301-85142c442bd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/?.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/?.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the saved state dictionary\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print the number of parameters in the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/?.pth'"
     ]
    }
   ],
   "source": [
    "name = '?'\n",
    "\n",
    "# Deserialize the JSON file back to a dictionary\n",
    "with open(f'models/{name}.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "# Convert the dictionary back to a dataclass object\n",
    "config = Config(**config_dict)\n",
    "\n",
    "# Initialize a blank model\n",
    "model = Model(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = f'models/{name}.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path)) \n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30e834-56f9-4812-ab4c-03e2f14874ca",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2260c4f-40b2-4b8e-9db4-ad1717a3027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou ouplayitx-. ennIigSlemWQarppribebePam8ttinayGozouonOUhTheyutGlo G;KWomroufandhaiheTheywststayall:Hplay?ppooz;;beanKKTheyeramsaDdari\n",
      "uO43\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\n",
    "max_useable_output_len = config.max_seq_len - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len, temperature=50.0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f258187-bf1d-482c-9c96-9cf21be21daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c0a4474-2503-452b-8054-e5d129557c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode('JULIET:\\nO Romeo, Romeo! wherefore art thou 9b\\nerAk!ot11EEerr$L$$$toplayplayplay2dwSou1waswas. sdha9ll,wasulTheJheomJJ, zplayplayTheyplayver?zEer;NP5000baThey?playplayplaver, stonmm4allCiPPha2CenenhiswawasirXandiMor,A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7ef80-36ef-4153-a1f2-de17fa396ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
