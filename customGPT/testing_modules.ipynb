{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea126987-59aa-4f76-b926-6d632887c30b",
   "metadata": {},
   "source": [
    "# This notebook is designed for teaching/testing purposes to help you visualize the tensor shapes that go through each module. Read it side-by-side with 'model.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c3f773-508c-4b13-8cfe-4f4b5a8907e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, '../venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c2c04f-2dbd-4020-8d91-cc0e4e8511b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "sys.path.append(\"..\")  # Adds the parent directory to the path so we can see the tokenizer\n",
    "from tokenizer_TinyStories import *\n",
    "size = 512 # size options are 128, 256, 512 and 1024\n",
    "path = f'../tokenizers/tiny_stories_tokenizer_{size}.model'\n",
    "tokenizer = get_tokenizer(path) \n",
    "\n",
    "# config file\n",
    "from config import *\n",
    "cfg = Config()\n",
    "cfg.vocab_len = tokenizer.vocab_len\n",
    "print(cfg)\n",
    "\n",
    "# model modules\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c12e3b-dc63-4479-ad55-b05d96364d1f",
   "metadata": {},
   "source": [
    "# Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627969e9-9017-43f3-90ec-9a485abef26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm type rmsnorm not found. defaulting to RMSNorm\n",
      "0.256 K parameters\n",
      "Norm()\n",
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "\n",
      "==========Entering Norm.RMSNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting Norm.RMSNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting Norm.forward==========\n"
     ]
    }
   ],
   "source": [
    "### RMSNorm\n",
    "\n",
    "# Create an instance of RMSNorm\n",
    "hold = cfg.norm_type\n",
    "cfg.norm_type = 'rmsnorm' # purposely mis-typing it\n",
    "module = Norm(cfg.dim, cfg)\n",
    "\n",
    "# let's take a look\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "cfg.norm_type = hold\n",
    "del hold, module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64020a96-205a-45b7-ae56-a3555d2b3719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "\n",
      "==========Entering Norm.LayerNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting Norm.LayerNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting Norm.forward==========\n"
     ]
    }
   ],
   "source": [
    "# LayerNorm\n",
    "hold = cfg.norm_type\n",
    "cfg.norm_type = 'LayerNorm'\n",
    "module = Norm(cfg.dim, cfg)\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim)\n",
    "output = module(x)\n",
    "module.disable_logging()\n",
    "cfg.norm_type = hold\n",
    "del hold, module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2d5014-3870-4f88-aee6-6de9de91bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "\n",
      "==========Entering Norm.CosineNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting Norm.CosineNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting Norm.forward==========\n"
     ]
    }
   ],
   "source": [
    "# CosineNorm\n",
    "hold = cfg.norm_type\n",
    "cfg.norm_type = 'CosineNorm'\n",
    "module = Norm(cfg.dim, cfg)\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim)\n",
    "output = module(x)\n",
    "module.disable_logging()\n",
    "cfg.norm_type = hold\n",
    "del hold, module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190f3de-37fd-442b-bfb1-6a090115fc75",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8f27b4-6d1f-4fcd-99b0-2284e65d6b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.96 K parameters\n",
      "MQSA(\n",
      "  (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (Wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "  (Wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "  (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
      ")\n",
      "\n",
      "==========Entering MQSA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "Tensor 'freqs_cis' shape: torch.Size([512, 16])\n",
      "Tensor 'mask' shape: torch.Size([512, 512])\n",
      "Integer 'cache_len': Value=True\n",
      "\n",
      "==========Entering MQSA.apply_rotary_emb==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([32, 512, 4, 32])\n",
      "Tensor 'xk' shape: torch.Size([32, 512, 1, 32])\n",
      "Tensor 'freqs_cis' shape: torch.Size([512, 16])\n",
      "\n",
      "==========Entering MQSA.reshape_for_broadcast==========\n",
      "Inputs:\n",
      "Tensor 'freqs_cis' shape: torch.Size([512, 16])\n",
      "Tensor 'x' shape: torch.Size([32, 512, 4, 16])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([1, 512, 1, 16])\n",
      "==========Exiting MQSA.reshape_for_broadcast==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 512, 4, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 512, 1, 32])\n",
      "==========Exiting MQSA.apply_rotary_emb==========\n",
      "\n",
      "==========Entering MQSA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'keys' shape: torch.Size([32, 512, 1, 32])\n",
      "Tensor 'values' shape: torch.Size([32, 512, 1, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 512, 4, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([32, 512, 4, 32])\n",
      "==========Exiting MQSA.match_headcount==========\n",
      "\n",
      "==========Entering MQSA.attend==========\n",
      "Inputs:\n",
      "Tensor 'queries' shape: torch.Size([32, 4, 512, 32])\n",
      "Tensor 'keys' shape: torch.Size([32, 4, 512, 32])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 4, 512, 512])\n",
      "==========Exiting MQSA.attend==========\n",
      "\n",
      "==========Entering MQSA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([32, 4, 512, 512])\n",
      "Tensor 'values' shape: torch.Size([32, 4, 512, 32])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting MQSA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting MQSA.forward==========\n"
     ]
    }
   ],
   "source": [
    "# first up let's look at training\n",
    "\n",
    "# Create an instance of multi-head self-attention\n",
    "module = MQSA(cfg)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('apply_rotary_emb')\n",
    "#module.disable_function_logging('reshape_for_broadcast')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "freqs_cis = precompute_freqs_cis(\n",
    "    cfg.dim // cfg.num_q_heads,\n",
    "    cfg.max_seq_len,\n",
    "    cfg.theta\n",
    ")\n",
    "mask = torch.full(\n",
    "    (cfg.max_seq_len, cfg.max_seq_len),\n",
    "    float(\"-inf\"),\n",
    "    device=cfg.device\n",
    ")\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x, freqs_cis, mask, training=True)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, freqs_cis, mask, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7e034c-c5a6-43ac-b6d2-bbb71f9965ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Entering MQSA.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([1, 64, 128])\n",
      "Tensor 'freqs_cis' shape: torch.Size([64, 16])\n",
      "Tensor 'mask' shape: torch.Size([64, 484])\n",
      "Integer 'cache_len': Value=420\n",
      "\n",
      "==========Entering MQSA.apply_rotary_emb==========\n",
      "Inputs:\n",
      "Tensor 'xq' shape: torch.Size([1, 64, 4, 32])\n",
      "Tensor 'xk' shape: torch.Size([1, 64, 1, 32])\n",
      "Tensor 'freqs_cis' shape: torch.Size([64, 16])\n",
      "\n",
      "==========Entering MQSA.reshape_for_broadcast==========\n",
      "Inputs:\n",
      "Tensor 'freqs_cis' shape: torch.Size([64, 16])\n",
      "Tensor 'x' shape: torch.Size([1, 64, 4, 16])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([1, 64, 1, 16])\n",
      "==========Exiting MQSA.reshape_for_broadcast==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 64, 4, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([1, 64, 1, 32])\n",
      "==========Exiting MQSA.apply_rotary_emb==========\n",
      "\n",
      "==========Entering MQSA.match_headcount==========\n",
      "Inputs:\n",
      "Tensor 'keys' shape: torch.Size([1, 484, 1, 32])\n",
      "Tensor 'values' shape: torch.Size([1, 484, 1, 32])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 484, 4, 32])\n",
      "Tensor 'output[1]' shape: torch.Size([1, 484, 4, 32])\n",
      "==========Exiting MQSA.match_headcount==========\n",
      "\n",
      "==========Entering MQSA.attend==========\n",
      "Inputs:\n",
      "Tensor 'queries' shape: torch.Size([1, 4, 64, 32])\n",
      "Tensor 'keys' shape: torch.Size([1, 4, 484, 32])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([1, 4, 64, 484])\n",
      "==========Exiting MQSA.attend==========\n",
      "\n",
      "==========Entering MQSA.calc_output==========\n",
      "Inputs:\n",
      "Tensor 'logits' shape: torch.Size([1, 4, 64, 484])\n",
      "Tensor 'values' shape: torch.Size([1, 4, 484, 32])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([1, 64, 128])\n",
      "==========Exiting MQSA.calc_output==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([1, 64, 128])\n",
      "==========Exiting MQSA.forward==========\n"
     ]
    }
   ],
   "source": [
    "# now let's do it for inference\n",
    "\n",
    "module = MQSA(cfg)\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('apply_rotary_emb')\n",
    "#module.disable_function_logging('reshape_for_broadcast')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "freqs_cis = precompute_freqs_cis(\n",
    "    cfg.dim // cfg.num_q_heads,\n",
    "    cfg.max_seq_len,\n",
    "    cfg.theta\n",
    ")\n",
    "mask = torch.full(\n",
    "    (cfg.max_seq_len, cfg.max_seq_len),\n",
    "    float(\"-inf\"),\n",
    "    device=cfg.device\n",
    ")\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "# setting up for kv caching\n",
    "cache_len = 420\n",
    "context_chunk_len = 64\n",
    "seq_len = cache_len + context_chunk_len\n",
    "# need to extend the mask with zeros for the cached values\n",
    "mask = mask[:context_chunk_len, :context_chunk_len]\n",
    "mask = torch.hstack(\n",
    "            [torch.zeros((context_chunk_len, cache_len)), mask]\n",
    "        )\n",
    "\n",
    "# these don't use seq_len because those entries should already be in the kv cache\n",
    "freqs_cis = freqs_cis[:context_chunk_len]\n",
    "x = torch.randn(1,context_chunk_len,cfg.dim)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x, freqs_cis, mask, cache_len)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, freqs_cis, mask, cache_len, context_chunk_len, seq_len, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb308c5-b578-46f2-86ae-bfa6800be641",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9b791e-d568-4b88-bdb3-41a22f274090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.304 K parameters\n",
      "MLP(\n",
      "  (Wgate): Linear(in_features=128, out_features=256, bias=False)\n",
      "  (Wup): Linear(in_features=128, out_features=256, bias=False)\n",
      "  (Wdown): Linear(in_features=256, out_features=128, bias=False)\n",
      "  (nonlinearity): GELU(approximate='none')\n",
      ")\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting MLP.forward==========\n"
     ]
    }
   ],
   "source": [
    "# GeGLU\n",
    "module = MLP(\n",
    "    cfg.dim, \n",
    "    cfg.dim * cfg.mlp_hidden_mult, \n",
    "    cfg.dim, \n",
    "    'GeLU', \n",
    "    gated=True, \n",
    "    bias=False, \n",
    "    dropout_rate = 0.1\n",
    ")\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "module.enable_logging()\n",
    "\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim)\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0748fa3-3230-4dd7-a768-1256ea72e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.536 K parameters\n",
      "MLP(\n",
      "  (Wup): Linear(in_features=128, out_features=256, bias=False)\n",
      "  (Wdown): Linear(in_features=256, out_features=128, bias=False)\n",
      "  (nonlinearity): ReLU()\n",
      ")\n",
      "\n",
      "==========Entering MLP.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting MLP.forward==========\n"
     ]
    }
   ],
   "source": [
    "# not gated, testing every other nonlinearity\n",
    "module = MLP(\n",
    "    cfg.dim, \n",
    "    cfg.dim * cfg.mlp_hidden_mult, \n",
    "    cfg.dim, \n",
    "    'ReLU', \n",
    "    gated=False, \n",
    "    bias=False, \n",
    "    dropout_rate = 0.1\n",
    ")\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "module.enable_logging()\n",
    "\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim)\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a502f-4646-4a02-9412-372482af9fa0",
   "metadata": {},
   "source": [
    "# ResidualLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20ad04be-06a1-4c26-9509-c9ad5bff5e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.776 K parameters\n",
      "ResidualLayer(\n",
      "  (pre_attn_norm): Norm()\n",
      "  (attn): MQSA(\n",
      "    (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (Wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "    (Wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "    (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "  )\n",
      "  (pre_mlp_norm): Norm()\n",
      "  (mlp): MLP(\n",
      "    (Wgate): Linear(in_features=128, out_features=256, bias=False)\n",
      "    (Wup): Linear(in_features=128, out_features=256, bias=False)\n",
      "    (Wdown): Linear(in_features=256, out_features=128, bias=False)\n",
      "    (nonlinearity): GELU(approximate='none')\n",
      "  )\n",
      ")\n",
      "\n",
      "==========Entering ResidualLayer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "Tensor 'freqs_cis' shape: torch.Size([512, 16])\n",
      "Tensor 'mask' shape: torch.Size([512, 512])\n",
      "Integer 'cache_len': Value=True\n",
      "\n",
      "==========Entering ResidualLayer.attn_connect==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "Tensor 'freqs_cis' shape: torch.Size([512, 16])\n",
      "Tensor 'mask' shape: torch.Size([512, 512])\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting ResidualLayer.attn_connect==========\n",
      "\n",
      "==========Entering ResidualLayer.mlp_connect==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting ResidualLayer.mlp_connect==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting ResidualLayer.forward==========\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "module = ResidualLayer(cfg)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('attn_connect')\n",
    "#module.disable_function_logging('mlp_connect')\n",
    "### enabling printing for sub-modules\n",
    "#module.pre_attn_norm.enable_logging()\n",
    "#module.attn.enable_logging()\n",
    "#module.post_attn_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "freqs_cis = precompute_freqs_cis(\n",
    "    cfg.dim // cfg.num_q_heads,\n",
    "    cfg.max_seq_len,\n",
    "    cfg.theta\n",
    ")\n",
    "mask = torch.full(\n",
    "    (cfg.max_seq_len, cfg.max_seq_len),\n",
    "    float(\"-inf\"),\n",
    "    device=cfg.device\n",
    ")\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "x = torch.randn(32,cfg.max_seq_len,cfg.dim)\n",
    "\n",
    "output = module(x, freqs_cis, mask, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f534ab4-a150-42db-8c89-5e8d3d0c06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.776 K parameters\n",
      "ResidualLayer(\n",
      "  (pre_attn_norm): Norm()\n",
      "  (attn): MQSA(\n",
      "    (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (Wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "    (Wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "    (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "  )\n",
      "  (pre_mlp_norm): Norm()\n",
      "  (mlp): MLP(\n",
      "    (Wgate): Linear(in_features=128, out_features=256, bias=False)\n",
      "    (Wup): Linear(in_features=128, out_features=256, bias=False)\n",
      "    (Wdown): Linear(in_features=256, out_features=128, bias=False)\n",
      "    (nonlinearity): GELU(approximate='none')\n",
      "  )\n",
      ")\n",
      "\n",
      "==========Entering ResidualLayer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([1, 64, 128])\n",
      "Tensor 'freqs_cis' shape: torch.Size([64, 16])\n",
      "Tensor 'mask' shape: torch.Size([64, 484])\n",
      "Integer 'cache_len': Value=420\n",
      "\n",
      "==========Entering ResidualLayer.attn_connect==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([1, 64, 128])\n",
      "Tensor 'freqs_cis' shape: torch.Size([64, 16])\n",
      "Tensor 'mask' shape: torch.Size([64, 484])\n",
      "Integer 'cache_len': Value=420\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([1, 64, 128])\n",
      "==========Exiting ResidualLayer.attn_connect==========\n",
      "\n",
      "==========Entering ResidualLayer.mlp_connect==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([1, 64, 128])\n",
      "Integer 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([1, 64, 128])\n",
      "==========Exiting ResidualLayer.mlp_connect==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([1, 64, 128])\n",
      "==========Exiting ResidualLayer.forward==========\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "module = ResidualLayer(cfg)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('attn_connect')\n",
    "#module.disable_function_logging('mlp_connect')\n",
    "#module.pre_attn_norm.enable_logging()\n",
    "#module.attn.enable_logging()\n",
    "#module.post_attn_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "freqs_cis = precompute_freqs_cis(\n",
    "    cfg.dim // cfg.num_q_heads,\n",
    "    cfg.max_seq_len,\n",
    "    cfg.theta\n",
    ")\n",
    "mask = torch.full(\n",
    "    (cfg.max_seq_len, cfg.max_seq_len),\n",
    "    float(\"-inf\"),\n",
    "    device=cfg.device\n",
    ")\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "# setting up for kv caching\n",
    "cache_len = 420\n",
    "context_chunk_len = 64\n",
    "seq_len = cache_len + context_chunk_len\n",
    "# need to extend the mask with zeros for the cached values\n",
    "mask = mask[:context_chunk_len, :context_chunk_len]\n",
    "mask = torch.hstack(\n",
    "            [torch.zeros((context_chunk_len, cache_len)), mask]\n",
    "        )\n",
    "# these don't use seq_len because those entries should already be in the kv cache\n",
    "freqs_cis = freqs_cis[:context_chunk_len]\n",
    "x = torch.randn(1,context_chunk_len,cfg.dim)\n",
    "\n",
    "output = module(x, freqs_cis, mask, cache_len)\n",
    "module.disable_logging()\n",
    "del module, freqs_cis, mask, cache_len, context_chunk_len, seq_len, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677ac2b-06d0-4895-b718-2bc664613c98",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7742b34d-4433-407e-8b87-f5b92af59433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463.552 K parameters\n",
      "customGPT(\n",
      "  (token_embedder): Embedding(512, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-9): 10 x ResidualLayer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQSA(\n",
      "        (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (Wup): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=128, bias=False)\n",
      "        (nonlinearity): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "==========Entering customGPT.forward==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([32, 512])\n",
      "Tensor 'cache_len' shape: torch.Size([32, 512])\n",
      "\n",
      "==========Entering ResidualLayer.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "Tensor 'freqs_cis' shape: torch.Size([512, 16])\n",
      "Tensor 'mask' shape: torch.Size([512, 512])\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Integer 'training': Value=True\n",
      "\n",
      "==========Entering ResidualLayer.attn_connect==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "Tensor 'freqs_cis' shape: torch.Size([512, 16])\n",
      "Tensor 'mask' shape: torch.Size([512, 512])\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting ResidualLayer.attn_connect==========\n",
      "\n",
      "==========Entering ResidualLayer.mlp_connect==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "Integer 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting ResidualLayer.mlp_connect==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting ResidualLayer.forward==========\n",
      "\n",
      "==========Entering Norm.forward==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "\n",
      "==========Entering Norm.RMSNorm==========\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([32, 512, 128])\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting Norm.RMSNorm==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([32, 512, 128])\n",
      "==========Exiting Norm.forward==========\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([32, 512, 512])\n",
      "Tensor 'output[1]' shape: torch.Size([])\n",
      "==========Exiting customGPT.forward==========\n",
      "tensor(127.0032, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "module = customGPT(cfg)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "module.layers[0].enable_logging()\n",
    "module.final_norm.enable_logging()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (32, cfg.max_seq_len))\n",
    "target_token_ids = torch.randint(tokenizer.vocab_len, (32, cfg.max_seq_len))\n",
    "\n",
    "output, loss = module(input_token_ids, target_token_ids=target_token_ids)\n",
    "print(loss)\n",
    "module.disable_logging()\n",
    "del module, input_token_ids, target_token_ids, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316598a6-1ba9-4a26-962f-16c0802a698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463.552 K parameters\n",
      "customGPT(\n",
      "  (token_embedder): Embedding(512, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-9): 10 x ResidualLayer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQSA(\n",
      "        (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (Wup): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=128, bias=False)\n",
      "        (nonlinearity): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "==========Entering customGPT.forward==========\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([1, 170])\n",
      "Integer 'cache_len': Value=69\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([1, 170, 512])\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "==========Exiting customGPT.forward==========\n",
      "torch.Size([1, 170, 512])\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "module = customGPT(cfg)\n",
    "print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "#for i in range(cfg.num_layers):\n",
    "    #module.layers[i].enable_logging()\n",
    "#module.final_norm.enable_logging()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (1, cfg.max_seq_len // 3))\n",
    "\n",
    "output, _ = module(input_token_ids, cache_len = 69)\n",
    "print(output.shape)\n",
    "module.disable_logging()\n",
    "del module, input_token_ids, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d82f9c-e531-47b5-8ffb-c8374ffb0e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
