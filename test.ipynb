{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c886b25-6c50-4f65-ac3d-b443b4da14ad",
   "metadata": {},
   "source": [
    "so the goal here is to figure out how to dynamically import functions and classes from different models within the sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc18d51d-b302-45f8-8333-d477811a4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "def import_from_nested_path(folders, file, items):\n",
    "    try:\n",
    "        # Construct the module path from a list of folders\n",
    "        module_path = \".\".join(folders) + \".\" + file\n",
    "        \n",
    "        # Dynamically import the module\n",
    "        module = importlib.import_module(module_path)\n",
    "        \n",
    "        # Extract specific items (functions, classes, etc.)\n",
    "        imported_items = {}\n",
    "        for item in items:\n",
    "            if hasattr(module, item):\n",
    "                imported_items[item] = getattr(module, item)\n",
    "            else:\n",
    "                print(f\"{item} is not available in {module_path}\")\n",
    "        return imported_items\n",
    "                \n",
    "    except ImportError as e:\n",
    "        print(f\"Failed to import module: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7fc9e9-887c-43b8-91cd-b027f40fb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_objects = import_from_nested_path(['models', 'customGPT'], 'inference', ['generate'])\n",
    "generate = imported_objects.get('generate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b70c63-1a5d-4a5f-bee5-47e749c242e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_objects = import_from_nested_path(['models', 'customGPT'], 'tools', ['load_model'])\n",
    "load_model = imported_objects.get('load_model')\n",
    "\n",
    "imported_objects = import_from_nested_path(['models', 'customGPT', 'tokenizers', 'bpe'], 'tokenizer', ['get_tokenizer'])\n",
    "get_tokenizer = imported_objects.get('get_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dad3cdff-edd6-4b8d-b2aa-44947ffabbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/tunadorable/local-repos/micro-GPT-sandbox/models/customGPT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def run_in_directory(func, path, *args, **kwargs):\n",
    "    original_dir = os.getcwd()  # Save the current directory\n",
    "    os.chdir(path)  # Change to the target directory\n",
    "    try:\n",
    "        result = func(*args, **kwargs)  # Execute the function\n",
    "    finally:\n",
    "        os.chdir(original_dir)  # Change back to the original directory\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "def example_function():\n",
    "    print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Calling the function with a custom directory\n",
    "run_in_directory(example_function, \"models/customGPT/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e19e932-b48d-409d-b9c4-e935902a34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "def load_model(\n",
    "    name: str, # the filepath to the model. ex: 'models/customGPT/trained/customGPT_0.5m_tall_and_skinny'\n",
    "    tokenizer_name: str, # the name of the folder with the tokenizer in it. 'bpe' in customGPT\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "):\n",
    "    path_parts = name.split('/')\n",
    "\n",
    "    # grabbing the config and Model class from the correct directories\n",
    "    def internal():\n",
    "        from modules.model import Model\n",
    "        from config import ModelConfig\n",
    "        return Model, ModelConfig\n",
    "    Model, ModelConfig = run_in_directory(internal, os.path.join(path_parts[0], path_parts[1]))\n",
    "    \n",
    "    # grabbing the get_tokenizer function from the correct directory\n",
    "    imported_objects = import_from_nested_path(\n",
    "        [path_parts[0], path_parts[1], 'tokenizers', tokenizer_name], \n",
    "        'tokenizer', \n",
    "        ['get_tokenizer']\n",
    "    )\n",
    "    get_tokenizer = imported_objects.get('get_tokenizer')\n",
    "\n",
    "    # Deserialize the JSON file back to a dictionary\n",
    "    with open(f'{name}/model_config.json', 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "    \n",
    "    # Convert the dictionary back to a Config object\n",
    "    cfg = ModelConfig(**config_dict)\n",
    "    cfg.device = device\n",
    "    \n",
    "    # defining the tokenizer\n",
    "    vocab_size = cfg.vocab_len - 3\n",
    "    tokenizer = run_in_directory(get_tokenizer, os.path.join(path_parts[0], path_parts[1]), vocab_size)\n",
    "    \n",
    "    # Initialize a blank model\n",
    "    model = Model(cfg).to(cfg.device) \n",
    "    \n",
    "    # Load the saved model parameters\n",
    "    model_path = os.path.join(path_parts[2], path_parts[3], 'model.pth')\n",
    "    run_in_directory(\n",
    "        lambda: model.load_state_dict(\n",
    "            torch.load(\n",
    "                model_path, \n",
    "                map_location=cfg.device\n",
    "            )\n",
    "        ), \n",
    "        os.path.join(path_parts[0], path_parts[1])\n",
    "    )\n",
    "    \n",
    "    print(f'{sum(p.numel() for p in model.parameters())/1e3}K parameters\\n{cfg}\\n{model}')\n",
    "\n",
    "    return model, tokenizer, cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86620330-47fd-4b2c-b23c-288445c06c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = [\n",
    "    'models/customGPT/trained/customGPT_0.5m_tall_and_skinny',\n",
    "    'models/customGPT/trained/customGPT_0.5m_5foot11_and_skinnyfat',\n",
    "    'models/customGPT/trained/customGPT_0.5m_short_and_thick',\n",
    "    'models/fractal-head-attention/trained/FHA_GPT_0.3m_2024-05-07|13-05-29',\n",
    "    'models/fractal-head-attention/trained/FHA_GPT_0.8m_2024-05-05|10-54-35'\n",
    "]\n",
    "tokenizer_list = ['bpe']*len(models_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9fc640f-9007-47b1-a294-0c21b8e10040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502.592K parameters\n",
      "ModelConfig(dim=64, vocab_len=2051, device='cpu', num_layers=10, second_resid_norm=False, mlp_hidden_mult=1, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=10, num_kv_heads=2, head_dim=16, theta=10000, max_seq_len=512, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n",
      "Model(\n",
      "  (token_embedder): Embedding(2051, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-9): 10 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=64, out_features=160, bias=False)\n",
      "        (Wk): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=160, out_features=64, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (Wup): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (Wdown): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "493.888K parameters\n",
      "ModelConfig(dim=64, vocab_len=2051, device='cpu', num_layers=8, second_resid_norm=False, mlp_hidden_mult=2, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=8, num_kv_heads=2, head_dim=16, theta=10000, max_seq_len=512, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n",
      "Model(\n",
      "  (token_embedder): Embedding(2051, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=128, out_features=64, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wup): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wdown): Linear(in_features=128, out_features=64, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "492.864K parameters\n",
      "ModelConfig(dim=64, vocab_len=2051, device='cpu', num_layers=4, second_resid_norm=False, mlp_hidden_mult=4, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=16, num_kv_heads=4, head_dim=16, theta=10000, max_seq_len=512, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n",
      "Model(\n",
      "  (token_embedder): Embedding(2051, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wk): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (Wv): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (Wo): Linear(in_features=256, out_features=64, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wup): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=64, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "312.64K parameters\n",
      "ModelConfig(dim=64, vocab_len=2051, device='cpu', num_layers=4, second_resid_norm=False, mlp_hidden_mult=2, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=4, num_kv_heads=1, head_dim=32, theta=10000, max_seq_len=256, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n",
      "Model(\n",
      "  (token_embedder): Embedding(2051, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=128, out_features=64, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wup): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wdown): Linear(in_features=128, out_features=64, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "821.888K parameters\n",
      "ModelConfig(dim=128, vocab_len=2051, device='cpu', num_layers=4, second_resid_norm=False, mlp_hidden_mult=2, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=4, num_kv_heads=1, head_dim=32, theta=10000, max_seq_len=256, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n",
      "Model(\n",
      "  (token_embedder): Embedding(2051, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (Wup): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=128, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for model_name, tokenizer in zip(models_to_compare, tokenizer_list):\n",
    "    model, _, _ = load_model(model_name, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06ecd1b-f8ba-4c78-a92f-ddb75a249b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== customGPT_0.5m_tall_and_skinny ====================\n",
      "502.592K parameters\n",
      "ModelConfig(dim=64, vocab_len=2051, device='cpu', num_layers=10, second_resid_norm=False, mlp_hidden_mult=1, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=10, num_kv_heads=2, head_dim=16, theta=10000, max_seq_len=512, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n",
      "Model(\n",
      "  (token_embedder): Embedding(2051, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-9): 10 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=64, out_features=160, bias=False)\n",
      "        (Wk): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=160, out_features=64, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (Wup): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (Wdown): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "==================== customGPT_0.5m_5foot11_and_skinnyfat ====================\n",
      "493.888K parameters\n",
      "ModelConfig(dim=64, vocab_len=2051, device='cpu', num_layers=8, second_resid_norm=False, mlp_hidden_mult=2, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=8, num_kv_heads=2, head_dim=16, theta=10000, max_seq_len=512, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n",
      "Model(\n",
      "  (token_embedder): Embedding(2051, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=128, out_features=64, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wup): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wdown): Linear(in_features=128, out_features=64, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "==================== customGPT_0.5m_short_and_thick ====================\n",
      "492.864K parameters\n",
      "ModelConfig(dim=64, vocab_len=2051, device='cpu', num_layers=4, second_resid_norm=False, mlp_hidden_mult=4, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=16, num_kv_heads=4, head_dim=16, theta=10000, max_seq_len=512, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n",
      "Model(\n",
      "  (token_embedder): Embedding(2051, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wk): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (Wv): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (Wo): Linear(in_features=256, out_features=64, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wup): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=64, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "==================== FHA_GPT_0.3m_2024-05-07|13-05-29 ====================\n",
      "312.64K parameters\n",
      "ModelConfig(dim=64, vocab_len=2051, device='cpu', num_layers=4, second_resid_norm=False, mlp_hidden_mult=2, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=4, num_kv_heads=1, head_dim=32, theta=10000, max_seq_len=256, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n",
      "Model(\n",
      "  (token_embedder): Embedding(2051, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=64, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=128, out_features=64, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wup): Linear(in_features=64, out_features=128, bias=False)\n",
      "        (Wdown): Linear(in_features=128, out_features=64, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "==================== FHA_GPT_0.8m_2024-05-05|10-54-35 ====================\n",
      "821.888K parameters\n",
      "ModelConfig(dim=128, vocab_len=2051, device='cpu', num_layers=4, second_resid_norm=False, mlp_hidden_mult=2, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=4, num_kv_heads=1, head_dim=32, theta=10000, max_seq_len=256, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n",
      "Model(\n",
      "  (token_embedder): Embedding(2051, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (Wup): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=128, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "models_to_compare = [\n",
    "    'models/customGPT/trained/customGPT_0.5m_tall_and_skinny',\n",
    "    'models/customGPT/trained/customGPT_0.5m_5foot11_and_skinnyfat',\n",
    "    'models/customGPT/trained/customGPT_0.5m_short_and_thick',\n",
    "    'models/fractal-head-attention/trained/FHA_GPT_0.3m_2024-05-07|13-05-29',\n",
    "    'models/fractal-head-attention/trained/FHA_GPT_0.8m_2024-05-05|10-54-35'\n",
    "]\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    path_parts = model_name.split('/')\n",
    "\n",
    "    imported_objects = import_from_nested_path(['models', 'customGPT'], 'tools', ['load_model'])\n",
    "    load_model = imported_objects.get('load_model')\n",
    "\n",
    "    print('='*20, path_parts[3], '='*20)\n",
    "    model, _, _ = run_in_directory(load_model, os.path.join(path_parts[0], path_parts[1]), path_parts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6cd05c-4682-44d6-8c46-3425bc95aef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
