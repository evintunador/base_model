{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8178ba-2ea0-4f4f-b9ca-c3435a83bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, './venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eae0015-a5c6-493e-832a-0cfcb0ef128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model modules\n",
    "from model import *\n",
    "\n",
    "# inference code\n",
    "from inference import *\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918a651-5c6a-4a39-8b3e-a28259e4fd64",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c0ba50-83de-4ad7-b262-944e6d547ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914.496 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "customGPT(\n",
       "  (token_embedder): Embedding(1027, 64)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x ResidualLayer(\n",
       "      (pre_attn_norm): Norm()\n",
       "      (attn): MQSA(\n",
       "        (Wq): Linear(in_features=64, out_features=128, bias=False)\n",
       "        (Wk): Linear(in_features=64, out_features=32, bias=False)\n",
       "        (Wv): Linear(in_features=64, out_features=32, bias=False)\n",
       "        (Wo): Linear(in_features=128, out_features=64, bias=False)\n",
       "      )\n",
       "      (post_attn_norm): Norm()\n",
       "      (pre_mlp_norm): Norm()\n",
       "      (mlp): MLP(\n",
       "        (Wgate): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (Wup): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (Wdown): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (nonlinearity): GELU(approximate='none')\n",
       "      )\n",
       "      (post_mlp_norm): Norm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): Norm()\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained model options:\n",
    "# - a 1.5m parameter that hasn't really been trained, just a test: customGPT_2024-04-25|10-16-11\n",
    "# - a 1m parameter model trained for 500 iters with warmup 50, lr_init 0.01, lr_final 1e-5, weight_decay 0.02: customGPT_2024-04-25|15-58-37\n",
    "name = 'customGPT_2024-04-25|15-58-37'\n",
    "\n",
    "# Deserialize the JSON file back to a dictionary\n",
    "with open(f'models/{name}.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "# Convert the dictionary back to a Config object\n",
    "from config import *\n",
    "cfg = Config(**config_dict)\n",
    "cfg.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# tokenizer\n",
    "from tokenizer import *\n",
    "size = cfg.vocab_len # size options are 128, 256, 512 and 1024\n",
    "path = f'./tokenizers/tiny_stories_tokenizer_{size-3}.model'\n",
    "tokenizer = get_tokenizer(path) \n",
    "\n",
    "# Initialize a blank model\n",
    "model = customGPT(cfg).to(cfg.device)  \n",
    "\n",
    "# Load the saved state dictionary\n",
    "path = f'models/{name}.pth'\n",
    "model.load_state_dict(torch.load(path)) \n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c971fce-8b3e-4732-bd66-d5d2028025d6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af2d78a-1d5b-42eb-85ad-0e486deb314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little boy named Tim. Tim loved to play with his toys. One day, Tim found a big dog came to the park. Tim was very happy and you wanted to help it. Tim was very happy. Tim was very happy and and played with it. Tim was happy to do the day, but they got to play with the ball. Tim was very happy and came on the ball and went to play with the ball.\n",
      "Tim was very happy and put the tower to the floor. The dog was very happy and the bird was too clapp\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once\"\n",
    "output = generate(\n",
    "    prompt, \n",
    "    model, \n",
    "    tokenizer,\n",
    "    #max_gen_len = 100,\n",
    "    #temperature = 0.7,\n",
    "    #memory_saver_div = 8,\n",
    "    #top_p = 0.9,\n",
    "    #top_k = 32,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ff56cce-bf34-4dbf-9feb-ff8daccf3f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum attention matrix size in memory will be 64x512 rather than 512x512\n",
      "\n",
      "Once upon a time, there was a little boy named Tim. Tim was a green every day. He liked to play with his mom. One day, he saw a big cat and went to the butch. Tim was very happy and the ball. Tim was very sad and the butter to the park with the he on the store. Tim said, \"Hello, Tim, can is you make can and said, \"Let's play with the bird. I knew marst the table and said, \"No, Tim, this ball to make up and went to the magic and on the tree. They all played together and play with the ground and the bird were very happy. They were happy to the work and found a fun day to the bee with his friends.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once\"\n",
    "output = generate(\n",
    "    prompt, \n",
    "    model, \n",
    "    tokenizer,\n",
    "    #max_gen_len = 100,\n",
    "    #temperature = 0.7,\n",
    "    memory_saver_div = 8,\n",
    "    #top_p = 0.9,\n",
    "    #top_k = 32,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f991cea-9d2d-4382-8443-9a134d6c78d4",
   "metadata": {},
   "source": [
    "# perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ffc4d-1482-4be0-ba29-a3f2f7c5947d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
