


# my virtual environments are rarely properly connected to jupyter so this fixes that
import sys
import os
current_dir = os.getcwd()  # Get the current working directory
venv_dir = os.path.join(current_dir, '../venv') 
python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)
site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')
sys.path.append(site_packages_path) 


# tokenizer
sys.path.append("..")  # Adds the parent directory to the path so we can see the tokenizer
from tokenizer_TinyStories import *
size = 512 # size options are 128, 256, 512 and 1024
path = f'../tokenizers/tiny_stories_tokenizer_{size}.model'
tokenizer = get_tokenizer(path) 


# config file
from config import *
cfg = Config()
print(cfg, cfg.context_chunk)

# model modules
from model import *





### RMSNorm

# Create an instance of RMSNorm
hold = cfg.norm_type
cfg.norm_type = 'rmsnorm' # purposely mis-typing it
module = Norm(cfg.dim, cfg)

# Initially, logging is disabled by default
module.enable_logging()

### Optionally disabling printing for sub-functions
#module.disable_function_logging('CosineNorm')
#module.disable_function_logging('LayerNorm')
#module.disable_function_logging('RMSNorm')

x = torch.randn(32,cfg.max_seq_len,cfg.dim)

# Call the forward method - logging will occur
output = module(x)

# Disable logging. 
# This isn't actually necessary since we won't be using this object again but that's how you'd do it
module.disable_logging()

# clearing up ram jic we're training later
cfg.norm_type = hold
del hold, module, x, output


# LayerNorm
hold = cfg.norm_type
cfg.norm_type = 'LayerNorm'
module = Norm(cfg.dim, cfg)
module.enable_logging()

### Optionally disabling printing for sub-functions
#module.disable_function_logging('CosineNorm')
#module.disable_function_logging('LayerNorm')
#module.disable_function_logging('RMSNorm')

x = torch.randn(32,cfg.max_seq_len,cfg.dim)
output = module(x)
module.disable_logging()
cfg.norm_type = hold
del hold, module, x, output


# CosineNorm
hold = cfg.norm_type
cfg.norm_type = 'CosineNorm'
module = Norm(cfg.dim, cfg)
module.enable_logging()

### Optionally disabling printing for sub-functions
#module.disable_function_logging('CosineNorm')
#module.disable_function_logging('LayerNorm')
#module.disable_function_logging('RMSNorm')

x = torch.randn(32,cfg.max_seq_len,cfg.dim)
output = module(x)
module.disable_logging()
cfg.norm_type = hold
del hold, module, x, output





# Create an instance of multi-head self-attention
module = MQSA(cfg)

# Initially, logging is disabled by default
module.enable_logging()

### Optionally disabling printing for sub-functions
#module.disable_function_logging('apply_rotary_emb')
#module.disable_function_logging('reshape_for_broadcast')
#module.disable_function_logging('match_headcount')
#module.disable_function_logging('attend')
#module.disable_function_logging('calc_output')

# precompute RoPE frequencies, causal mask, and dummy input data
freqs_cis = precompute_freqs_cis(
    cfg.dim // cfg.num_q_heads,
    cfg.max_seq_len,
    cfg.theta
)
mask = torch.full(
    (cfg.max_seq_len, cfg.max_seq_len),
    float("-inf"),
    device=cfg.device
)
mask = torch.triu(mask, diagonal=1)
x = torch.randn(32,cfg.max_seq_len,cfg.dim)

# Call the forward method - logging will occur
output = module(x, freqs_cis, mask, training=True)

# Disable logging. 
# This isn't actually necessary since we won't be using this object again but that's how you'd do it
module.disable_logging()

# clearing up ram jic we're training later
del module, freqs_cis, mask, x, output


# now let's do it for inference

module = MQSA(cfg)
module.enable_logging()
#module.disable_function_logging('apply_rotary_emb')
#module.disable_function_logging('reshape_for_broadcast')
#module.disable_function_logging('match_headcount')
#module.disable_function_logging('attend')
#module.disable_function_logging('calc_output')

# precompute RoPE frequencies, causal mask, and dummy input data
freqs_cis = precompute_freqs_cis(
    cfg.dim // cfg.num_q_heads,
    cfg.max_seq_len,
    cfg.theta
)
mask = torch.full(
    (cfg.max_seq_len, cfg.max_seq_len),
    float("-inf"),
    device=cfg.device
)
mask = torch.triu(mask, diagonal=1)

# setting up for kv caching
cache_len = 420
seqlen = cache_len + cfg.context_chunk()
# need to extend the mask with zeros for the cached values
mask = mask[:cfg.context_chunk(), :cfg.context_chunk()]
mask = torch.hstack(
            [torch.zeros((cfg.context_chunk(), cache_len)), mask]
        )

# these don't use seqlen because those entries should already be in the kv cache
freqs_cis = freqs_cis[:cfg.context_chunk()]
x = torch.randn(32,cfg.context_chunk(),cfg.dim)

# Call the forward method - logging will occur
output = module(x, freqs_cis, mask, cache_len)

# Disable logging. 
# This isn't actually necessary since we won't be using this object again but that's how you'd do it
module.disable_logging()

# clearing up ram jic we're training later
del module, freqs_cis, mask, cache_len, seqlen, x, output






