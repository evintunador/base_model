{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba617fb-d786-4169-8d39-2a97145c9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import regex as re\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5d3ca2-7422-48f1-b39d-d4229700fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data.pkl', 'rb') as file:\n",
    "\tlist_of_strings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fed68fd-7c03-4d2e-92e4-dd3a2d0d7fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2732634\n",
      "Once upon a time, there was a little girl named Sue. Sue was very adventurous. She loved to play outside and explore. One day, Sue saw a big box in her room. She did not know what was inside.\n",
      "Sue's mommy said, \"Sue, sit down and open the box.\" Sue sat down and opened the box. Inside, she found a beautiful uniform. It was red and blue with gold stars. Sue put on the uniform and felt very special.\n",
      "Sue wore her uniform outside to play. She met her friend Tom. Tom said, \"Wow, Sue! Your uniform is so pretty!\" Sue smiled and said, \"Thank you, Tom. Let's go on an adventure!\" Together, they explored the park and had a great day. Sue knew that her special uniform made her feel even more adventurous.\n",
      "699\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_strings))\n",
    "print(list_of_strings[0])\n",
    "print(len(list_of_strings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2133207-6ee0-4f47-8018-da1a867c5136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['\\n', ' ', '!', '\"', '$', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 74\n"
     ]
    }
   ],
   "source": [
    "# turn it into one string instead of a list of strings\n",
    "combined_string = ''.join(list_of_strings)\n",
    "\n",
    "# find the unique characters\n",
    "chars = sorted(list(set(combined_string)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)\n",
    "del combined_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3856169e-9b04-440b-9119-70832902bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "char_encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba69cd54-8390-4355-881a-cd2f233922dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 128 # the desired final vocabulary size\n",
    "num_merges = vocab_size - v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "208c4957-cec2-4936-8147-73c8a55dc2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]\n",
      "['symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter']\n"
     ]
    }
   ],
   "source": [
    "base_indices = char_encode(chars)\n",
    "print(base_indices)\n",
    "origin = [ \"symbol\" if i < 22 else \"letter\" for i in base_indices]  # Track token origin\n",
    "print(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0e7fcb6-5927-45b9-b6a9-23ce1df8f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba933bfb-026c-47f7-8fff-42868bf5818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (55, 52) into a new token 74\n",
      "merging (8, 1) into a new token 75\n",
      "merging (48, 61) into a new token 76\n",
      "merging (67, 74) into a new token 77\n",
      "merging (52, 51) into a new token 78\n",
      "merging (6, 1) into a new token 79\n",
      "merging (67, 62) into a new token 80\n",
      "merging (76, 51) into a new token 81\n",
      "merging (56, 61) into a new token 82\n",
      "merging (65, 52) into a new token 83\n",
      "merging (62, 68) into a new token 84\n",
      "merging (70, 48) into a new token 85\n",
      "merging (56, 67) into a new token 86\n",
      "merging (55, 48) into a new token 87\n",
      "merging (41, 74) into a new token 88\n",
      "merging (52, 65) into a new token 89\n",
      "merging (85, 66) into a new token 90\n",
      "merging (48, 72) into a new token 91\n",
      "merging (52, 61) into a new token 92\n",
      "merging (62, 60) into a new token 93\n",
      "merging (56, 66) into a new token 94\n",
      "merging (62, 61) into a new token 95\n",
      "merging (56, 60) into a new token 96\n",
      "merging (48, 65) into a new token 97\n",
      "merging (66, 48) into a new token 98\n",
      "merging (56, 51) into a new token 99\n",
      "merging (59, 52) into a new token 100\n",
      "merging (59, 59) into a new token 101\n",
      "merging (82, 54) into a new token 102\n",
      "merging (62, 67) into a new token 103\n",
      "merging (62, 65) into a new token 104\n",
      "merging (66, 67) into a new token 105\n",
      "merging (56, 65) into a new token 106\n",
      "merging (8, 0) into a new token 107\n",
      "merging (48, 60) into a new token 108\n",
      "merging (61, 52) into a new token 109\n",
      "merging (88, 72) into a new token 110\n",
      "merging (63, 59) into a new token 111\n",
      "merging (48, 67) into a new token 112\n",
      "merging (56, 59) into a new token 113\n",
      "merging (59, 62) into a new token 114\n",
      "merging (56, 54) into a new token 115\n",
      "merging (29, 52) into a new token 116\n",
      "merging (69, 89) into a new token 117\n",
      "merging (49, 52) into a new token 118\n",
      "merging (55, 94) into a new token 119\n",
      "merging (111, 91) into a new token 120\n",
      "merging (65, 56) into a new token 121\n",
      "merging (51, 91) into a new token 122\n",
      "merging (41, 96) into a new token 123\n",
      "merging (68, 67) into a new token 124\n",
      "merging (48, 101) into a new token 125\n",
      "merging (74, 65) into a new token 126\n",
      "merging (63, 63) into a new token 127\n"
     ]
    }
   ],
   "source": [
    "# turning a usable subset of the data into one string\n",
    "combined_string = ''.join(list_of_strings[:1000])\n",
    "tokens = char_encode(combined_string)\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "# now let's actually do it\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    #print(i)\n",
    "    stats = get_stats(ids)\n",
    "\n",
    "    # Modified pair selection logic:\n",
    "    while True:\n",
    "        pair = max(stats, key=stats.get)  # Get the most frequent pair initially\n",
    "        #print(pair)\n",
    "\n",
    "        if origin[pair[0]] != origin[pair[1]]: # Check if origins differ\n",
    "            #print(origin[pair[0]],origin[pair[1]],origin[pair[0]] != origin[pair[1]])\n",
    "            del stats[pair]\n",
    "        else:  # If no valid pairs left, break out of the loop\n",
    "            #print(origin[pair[0]],origin[pair[1]],origin[pair[0]] != origin[pair[1]])\n",
    "            break\n",
    "    else:\n",
    "        break  # Valid pair found \n",
    "    \n",
    "    pair = max(stats, key=stats.get)\n",
    "    #print(pair)\n",
    "\n",
    "    idx = v + i\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "    origin.append(origin[pair[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ea99684-f232-4a8f-80f4-48b841633ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 804737\n",
      "ids length: 578026\n",
      "compression ratio: 1.39X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens)) # remember tokens are our original tokens\n",
    "print(\"ids length:\", len(ids)) # and ids are new tokens we've made\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "030522f4-ad67-4200-8598-261a9e1fa744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Ensure the tokenizers directory exists\n",
    "if not os.path.exists('./tokenizers'):\n",
    "    os.makedirs('./tokenizers')\n",
    "\n",
    "# Prepare the tokenizer data to be saved\n",
    "tokenizer_data = {\n",
    "    'stoi': stoi,  # Character to integer mapping\n",
    "    'merges': merges  # Merges dictionary\n",
    "}\n",
    "\n",
    "# Save the tokenizer data using pickle\n",
    "with open(f'./tokenizers/tiny_stories_tokenizer_{vocab_size}.model', 'wb') as f:\n",
    "    pickle.dump(tokenizer_data, f)\n",
    "\n",
    "print(\"Tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ebd18d9-6f8f-485f-826a-fb3645053e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer data using pickle\n",
    "with open(f'./tokenizers/tiny_stories_tokenizer_{vocab_size}.model', 'rb') as f:\n",
    "    loaded_tokenizer_data = pickle.load(f)\n",
    "\n",
    "# Extract the stoi mapping and merges from the loaded data\n",
    "loaded_stoi = loaded_tokenizer_data['stoi']\n",
    "loaded_merges = loaded_tokenizer_data['merges']\n",
    "\n",
    "print(\"Tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5d091ec-eff0-4900-9f82-3f47e4b4de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '4': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, ';': 20, '?': 21, 'A': 22, 'B': 23, 'C': 24, 'D': 25, 'E': 26, 'F': 27, 'G': 28, 'H': 29, 'I': 30, 'J': 31, 'K': 32, 'L': 33, 'M': 34, 'N': 35, 'O': 36, 'P': 37, 'Q': 38, 'R': 39, 'S': 40, 'T': 41, 'U': 42, 'V': 43, 'W': 44, 'X': 45, 'Y': 46, 'Z': 47, 'a': 48, 'b': 49, 'c': 50, 'd': 51, 'e': 52, 'f': 53, 'g': 54, 'h': 55, 'i': 56, 'j': 57, 'k': 58, 'l': 59, 'm': 60, 'n': 61, 'o': 62, 'p': 63, 'q': 64, 'r': 65, 's': 66, 't': 67, 'u': 68, 'v': 69, 'w': 70, 'x': 71, 'y': 72, 'z': 73}\n",
      "{(55, 52): 74, (8, 1): 75, (48, 61): 76, (67, 74): 77, (52, 51): 78, (6, 1): 79, (67, 62): 80, (76, 51): 81, (56, 61): 82, (65, 52): 83, (62, 68): 84, (70, 48): 85, (56, 67): 86, (55, 48): 87, (41, 74): 88, (52, 65): 89, (85, 66): 90, (48, 72): 91, (52, 61): 92, (62, 60): 93, (56, 66): 94, (62, 61): 95, (56, 60): 96, (48, 65): 97, (66, 48): 98, (56, 51): 99, (59, 52): 100, (59, 59): 101, (82, 54): 102, (62, 67): 103, (62, 65): 104, (66, 67): 105, (56, 65): 106, (8, 0): 107, (48, 60): 108, (61, 52): 109, (88, 72): 110, (63, 59): 111, (48, 67): 112, (56, 59): 113, (59, 62): 114, (56, 54): 115, (29, 52): 116, (69, 89): 117, (49, 52): 118, (55, 94): 119, (111, 91): 120, (65, 56): 121, (51, 91): 122, (41, 96): 123, (68, 67): 124, (48, 101): 125, (74, 65): 126, (63, 63): 127}\n"
     ]
    }
   ],
   "source": [
    "print(stoi)\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8609066b-4579-47ec-97a6-27b4435c038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [31, 42, 33, 30, 26, 41, 19, 0, 36, 1, 39, 93, 52, 62, 79, 39, 93, 52, 62, 2, 1, 70, 126, 52, 53, 104, 52, 1, 97, 67, 1, 67, 55, 84, 1, 39]\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou R\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, merges):\n",
    "        self.stoi = stoi\n",
    "        self.merges = merges\n",
    "        self.itos = {i: s for s, i in stoi.items()}  # Inverse mapping for decoding\n",
    "\n",
    "        self.vocab_len = len(stoi) + len(merges)\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert the text to a list of token IDs, using space for unknown characters\n",
    "        tokens = [self.stoi.get(c, self.stoi[' ']) for c in text]\n",
    "\n",
    "        # Perform merging with the possibility of nested merges\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in self.merges:\n",
    "                # Replace the current pair with its merged token\n",
    "                merged_token = self.merges[pair]\n",
    "                tokens[i] = merged_token\n",
    "                del tokens[i + 1]\n",
    "\n",
    "                # Move back to handle possible nested merges\n",
    "                if i > 0:\n",
    "                    i -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        def expand_token(token):\n",
    "            # Base case: if the token is a direct mapping, return its character\n",
    "            if token in self.itos:\n",
    "                return self.itos[token]\n",
    "            # Recursive case: if the token is a merged token, expand its constituents\n",
    "            elif token in self.merges.values():\n",
    "                pair = next(key for key, value in self.merges.items() if value == token)\n",
    "                return ''.join(expand_token(t) for t in pair)\n",
    "            # Fallback for unknown tokens\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        # Decode each token in the list, handling nested merges recursively\n",
    "        return ''.join(expand_token(token) for token in tokens)\n",
    "\n",
    "# Example usage\n",
    "# Assuming loaded_stoi and loaded_merges are already loaded from the tokenizer.model file\n",
    "\n",
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou R\")\n",
    "print(\"Encoded:\", encoded_text)\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ba18a30-c97c-41bc-a37b-23e82bd7d055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: '\n",
      "'\n",
      "1: ' '\n",
      "2: '!'\n",
      "3: '\"'\n",
      "4: '$'\n",
      "5: '''\n",
      "6: ','\n",
      "7: '-'\n",
      "8: '.'\n",
      "9: '0'\n",
      "10: '1'\n",
      "11: '2'\n",
      "12: '3'\n",
      "13: '4'\n",
      "14: '5'\n",
      "15: '6'\n",
      "16: '7'\n",
      "17: '8'\n",
      "18: '9'\n",
      "19: ':'\n",
      "20: ';'\n",
      "21: '?'\n",
      "22: 'A'\n",
      "23: 'B'\n",
      "24: 'C'\n",
      "25: 'D'\n",
      "26: 'E'\n",
      "27: 'F'\n",
      "28: 'G'\n",
      "29: 'H'\n",
      "30: 'I'\n",
      "31: 'J'\n",
      "32: 'K'\n",
      "33: 'L'\n",
      "34: 'M'\n",
      "35: 'N'\n",
      "36: 'O'\n",
      "37: 'P'\n",
      "38: 'Q'\n",
      "39: 'R'\n",
      "40: 'S'\n",
      "41: 'T'\n",
      "42: 'U'\n",
      "43: 'V'\n",
      "44: 'W'\n",
      "45: 'X'\n",
      "46: 'Y'\n",
      "47: 'Z'\n",
      "48: 'a'\n",
      "49: 'b'\n",
      "50: 'c'\n",
      "51: 'd'\n",
      "52: 'e'\n",
      "53: 'f'\n",
      "54: 'g'\n",
      "55: 'h'\n",
      "56: 'i'\n",
      "57: 'j'\n",
      "58: 'k'\n",
      "59: 'l'\n",
      "60: 'm'\n",
      "61: 'n'\n",
      "62: 'o'\n",
      "63: 'p'\n",
      "64: 'q'\n",
      "65: 'r'\n",
      "66: 's'\n",
      "67: 't'\n",
      "68: 'u'\n",
      "69: 'v'\n",
      "70: 'w'\n",
      "71: 'x'\n",
      "72: 'y'\n",
      "73: 'z'\n",
      "74: 'he'\n",
      "75: '. '\n",
      "76: 'an'\n",
      "77: 'the'\n",
      "78: 'ed'\n",
      "79: ', '\n",
      "80: 'to'\n",
      "81: 'and'\n",
      "82: 'in'\n",
      "83: 're'\n",
      "84: 'ou'\n",
      "85: 'wa'\n",
      "86: 'it'\n",
      "87: 'ha'\n",
      "88: 'The'\n",
      "89: 'er'\n",
      "90: 'was'\n",
      "91: 'ay'\n",
      "92: 'en'\n",
      "93: 'om'\n",
      "94: 'is'\n",
      "95: 'on'\n",
      "96: 'im'\n",
      "97: 'ar'\n",
      "98: 'sa'\n",
      "99: 'id'\n",
      "100: 'le'\n",
      "101: 'll'\n",
      "102: 'ing'\n",
      "103: 'ot'\n",
      "104: 'or'\n",
      "105: 'st'\n",
      "106: 'ir'\n",
      "107: '.\n",
      "'\n",
      "108: 'am'\n",
      "109: 'ne'\n",
      "110: 'They'\n",
      "111: 'pl'\n",
      "112: 'at'\n",
      "113: 'il'\n",
      "114: 'lo'\n",
      "115: 'ig'\n",
      "116: 'He'\n",
      "117: 'ver'\n",
      "118: 'be'\n",
      "119: 'his'\n",
      "120: 'play'\n",
      "121: 'ri'\n",
      "122: 'day'\n",
      "123: 'Tim'\n",
      "124: 'ut'\n",
      "125: 'all'\n",
      "126: 'her'\n",
      "127: 'pp'\n"
     ]
    }
   ],
   "source": [
    "for i in range(vocab_size):\n",
    "    print(f\"{i}: '{tokenizer.decode([i])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef4d05-1299-40f0-8fac-c4a1ac6ae6da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
