{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# imports for the tokenizer\n",
    "from tiny_stories_tokenizer import *\n",
    "\n",
    "# Imports used for the config\n",
    "import dataclasses \n",
    "from typing import Optional\n",
    "\n",
    "# for the dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "\n",
    "# used in the training loop\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd373bf-d326-4939-be18-819bae892805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(size = 128) # size options are 128, 256, 512 and 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ef54bc-7b2c-49f7-ae86-86d9c05b9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for next-concept predictor\n",
    "    \"\"\"\n",
    "    ### boring hyperparameters ###\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    max_seq_len: int = 512\n",
    "    num_hidden_layers: int = 4\n",
    "    num_q_heads: int = 4\n",
    "    num_kv_heads: int = 1 \n",
    "    assert num_q_heads % num_kv_heads == 0\n",
    "    embed_dim: int = 128 \n",
    "    mlp_multiplier: int = 4\n",
    "    head_dim: int = 32\n",
    "    theta = 100.0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd47c947-b56b-43db-9bc3-75c5af0253c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 <class 'list'> \n",
      "\n",
      " 892 One day, a little girl named Lucy went to the garden. It was a beautiful sunny day and the garden was full of colourful flowers. She was so excited to explore the garden! She ran around to find the most beautiful flower. \n",
      "As Lucy walked along, she noticed something strange. There was a big icy door in front of her. She wondered what was inside the door. She reached out her hand and touched the icy door. \"I think I can unlock the door,\" thought Lucy. \n",
      "Lucy twisted the door handle and the door opened! Suddenly, she saw a big, scary bear! The furry bear was roaring and came straight towards her. Lucy was very scared and started to cry. \n",
      "The bear chased Lucy until she could not run anymore. She tripped and the bear grabbed her in its big, strong arms. Lucy screamed for help, but no one was around to help her. \n",
      "The bear quickly locked Lucy in his icy cave and she was never seen again. \n",
      "\n",
      " 645 [36, 109, 1, 122, 79, 48, 1, 59, 86, 67, 100, 1, 54, 106, 59, 1, 61, 108, 78, 1, 33, 68, 50, 72, 1, 70, 92, 67, 1, 80, 1, 77, 1, 54, 97, 51, 92, 75, 30, 67, 1, 90, 1, 48, 1, 118, 48, 124, 56, 53, 68, 59, 1, 66, 68, 61, 61, 72, 1, 122, 1, 81, 1, 77, 1, 54, 97, 51, 92, 1, 90, 1, 53, 68, 101, 1, 62, 53, 1, 50, 62, 114, 68, 65, 53, 68, 59, 1, 53, 114, 70, 89, 66, 75, 40, 74, 1, 90, 1, 66, 62, 1, 52, 71, 50, 86, 78, 1, 80, 1, 52, 71, 111, 104, 52, 1, 77, 1, 54, 97, 51, 92, 2, 1, 40, 74, 1, 65, 76, 1, 97, 84, 61, 51, 1, 80, 1, 53, 82, 51, 1, 77, 1, 60, 62, 105, 1, 118, 48, 124, 56, 53, 68, 59, 1, 53, 114, 70, 89, 75, 0, 22, 66, 1, 33, 68, 50, 72, 1, 85, 59, 58, 78, 1, 48, 114, 61, 54, 79, 66, 74, 1, 61, 103, 56, 50, 78, 1, 66, 93, 52, 67, 55, 102, 1, 105, 65, 76, 54, 52, 75, 88, 83, 1, 90, 1, 48, 1, 49, 115, 1, 56, 50, 72, 1, 51, 62, 104, 1, 82, 1, 53, 65, 95, 67, 1, 62, 53, 1, 126, 75, 40, 74, 1, 70, 95, 51, 89, 78, 1, 70, 87, 67, 1, 90, 1, 82, 66, 99, 52, 1, 77, 1, 51, 62, 104, 75, 40, 74, 1, 83, 48, 50, 74, 51, 1, 84, 67, 1, 126, 1, 87, 61, 51, 1, 81, 1, 80, 68, 50, 74, 51, 1, 77, 1, 56, 50, 72, 1, 51, 62, 104, 75, 3, 30, 1, 67, 55, 82, 58, 1, 30, 1, 50, 76, 1, 68, 61, 114, 50, 58, 1, 77, 1, 51, 62, 104, 6, 3, 1, 67, 55, 84, 54, 55, 67, 1, 33, 68, 50, 72, 75, 0, 33, 68, 50, 72, 1, 67, 70, 94, 67, 78, 1, 77, 1, 51, 62, 104, 1, 87, 61, 51, 100, 1, 81, 1, 77, 1, 51, 62, 104, 1, 62, 63, 92, 78, 2, 1, 40, 68, 51, 51, 92, 59, 72, 79, 66, 74, 1, 98, 70, 1, 48, 1, 49, 115, 79, 66, 50, 97, 72, 1, 118, 97, 2, 1, 88, 1, 53, 68, 65, 65, 72, 1, 118, 97, 1, 90, 1, 65, 62, 97, 102, 1, 81, 1, 50, 108, 52, 1, 105, 65, 48, 115, 55, 67, 1, 80, 85, 65, 51, 66, 1, 126, 75, 33, 68, 50, 72, 1, 90, 1, 117, 72, 1, 66, 50, 97, 78, 1, 81, 1, 105, 97, 67, 78, 1, 80, 1, 50, 65, 72, 75, 0, 88, 1, 118, 97, 1, 50, 87, 66, 78, 1, 33, 68, 50, 72, 1, 68, 61, 67, 113, 1, 66, 74, 1, 50, 84, 59, 51, 1, 61, 103, 1, 65, 68, 61, 1, 76, 72, 60, 104, 52, 75, 40, 74, 1, 67, 121, 127, 78, 1, 81, 1, 77, 1, 118, 97, 1, 54, 65, 48, 49, 118, 51, 1, 126, 1, 82, 1, 86, 66, 1, 49, 115, 79, 105, 65, 95, 54, 1, 97, 60, 66, 75, 33, 68, 50, 72, 1, 66, 50, 83, 108, 78, 1, 53, 104, 1, 74, 59, 63, 79, 49, 124, 1, 61, 62, 1, 95, 52, 1, 90, 1, 97, 84, 61, 51, 1, 80, 1, 74, 59, 63, 1, 126, 75, 0, 88, 1, 118, 97, 1, 64, 68, 56, 50, 58, 59, 72, 1, 114, 50, 58, 78, 1, 33, 68, 50, 72, 1, 82, 1, 119, 1, 56, 50, 72, 1, 50, 48, 69, 52, 1, 81, 1, 66, 74, 1, 90, 1, 109, 117, 1, 66, 52, 92, 1, 48, 54, 48, 82, 8]\n"
     ]
    }
   ],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        # Store the file path\n",
    "        self.file_path = file_path\n",
    "        \n",
    "        # Open the pickle file and load the list of indices (or any structure that allows random access)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            self.data = pickle.load(file)\n",
    "        \n",
    "        # If your data is not a list but a single string, you need to preprocess it here to split it into samples\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of items in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the story at the given index\n",
    "        story = self.data[idx]\n",
    "        \n",
    "        # Here you would typically convert the story to a tensor, for example, by tokenizing it\n",
    "        # For simplicity, let's assume the story is already in the desired format\n",
    "        return story\n",
    "\n",
    "train_dataset = TinyStoriesDataset('tiny_stories_train_data.pkl')\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TinyStoriesDataset('tiny_stories_val_data.pkl')\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "train_loader_iter = iter(train_loader)\n",
    "val_loader_iter = iter(val_loader)\n",
    "next_batch = next(train_loader_iter)  # Fetch the next batch\n",
    "# Now, you can process `next_batch` as needed\n",
    "print(len(next_batch), type(next_batch), '\\n\\n', len(next_batch[0]), next_batch[0], '\\n\\n', len(tokenizer.encode(next_batch[0])), tokenizer.encode(next_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6384a84e-ca6c-4479-ab8c-4daf3d724b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoPE(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "    \n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181d512a-8fa4-4e54-a511-19e07a9fd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQA(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_q_heads = config.num_q_heads\n",
    "        self.num_kv_heads = config.num_kv_heads\n",
    "        assert self.num_q_heads % self.num_kv_heads == 0\n",
    "        self.num_queries_per_kv = self.num_q_heads // self.num_kv_heads\n",
    "\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.head_dim = config.head_dim\n",
    "        self.theta = config.theta\n",
    "\n",
    "        # Calculates the total size for all projections.\n",
    "        self.q_size = self.num_q_heads * self.head_dim\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "\n",
    "        # Defines the scaling factor for the attention scores.\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.embed_dim, (self.num_q_heads + 2 * self.num_kv_heads) * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_q_heads * self.head_dim, self.embed_dim, bias=False)\n",
    "    \n",
    "        # for our attention mask we'll create a boolean mask that'll later be turned into large negative values\n",
    "        self.mask = torch.tril(torch.ones((config.max_seq_len, config.max_seq_len), dtype=torch.uint8)\n",
    "                              ).view(1, 1, config.max_seq_len, config.max_seq_len).to(dtype=torch.bool)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len, _ = x.shape\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        # Splits the combined QKV tensor into separate tensors for queries (xq), keys (xk), and values (xv) based on their respective sizes.\n",
    "        xq, xk, xv = qkv.split([self.q_size, self.kv_size, self.kv_size],dim=-1)\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.num_q_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = RoPE(xq, self.head_dim, self.theta)\n",
    "        xk = RoPE(xk, self.head_dim, self.theta)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.num_kv_heads != self.num_q_heads:\n",
    "            xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2) # [batch_size, input_len, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        q = xq.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        k = xk.transpose(1, 2)\n",
    "        v = xv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        logits = torch.matmul(q, k.transpose(2, 3)) * self.scaling # [batch_size, n_local_heads, input_len, input_len]\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention logits\n",
    "        logits = torch.where(self.mask[..., :input_len, :input_len].expand_as(logits), \n",
    "                             logits, \n",
    "                             torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "\n",
    "        # Applies softmax to the logits to obtain attention probabilities\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        output = torch.matmul(scores, v) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1) # [batch_size, input_len, hidden_dim]\n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3111446c-52ff-4c36-b441-e6724d6ad461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gate_proj = nn.Linear(embed_dim, hidden_size)\n",
    "        self.up_proj = nn.Linear(embed_dim, hidden_size)\n",
    "        self.down_proj = nn.Linear(hidden_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.up_proj(x) * F.gelu(self.gate_proj(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937ec822-e4fb-4b76-9ada-37dd1f94acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, use_scale=True):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(num_features)) if use_scale else None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the mean squared value for each feature\n",
    "        mean_squared = inputs.pow(2).mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # Normalize inputs\n",
    "        normed_inputs = inputs * torch.rsqrt(mean_squared + self.eps)\n",
    "\n",
    "        # Apply scale if it exists\n",
    "        if self.scale is not None:\n",
    "            normed_inputs = normed_inputs * self.scale\n",
    "\n",
    "        return normed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbd1fcf1-a1dd-4e6c-8d78-e267398304f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mqa = MQA(config)\n",
    "        self.mlp = MLP(config.embed_dim, config.embed_dim * config.mlp_multiplier)\n",
    "        \n",
    "        self.pre_mqa_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "        self.post_mqa_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "        self.pre_mlp_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "        self.post_mlp_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor ) -> torch.Tensor:\n",
    "        x = x + self.post_mqa_norm(self.mqa(self.pre_mqa_norm(x)))\n",
    "        x = x + self.post_mlp_norm(self.mlp(self.pre_mlp_norm(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dea80b7-2d2e-4e0d-9616-6acfee9b4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customGPT(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        config: Config, # the hyperparameters\n",
    "        tokenizer: tokenizer, # the tokenizer. we don't always store the tokenizer inside of the model, but it doesn't matter here\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # the attention heads need to cleanly divide up the embed_dim of the model so that we can split it all apart & combine back together\n",
    "        assert config.embed_dim % config.num_q_heads == 0\n",
    "\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.head_dim = config.head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(self.vocab_size+1, config.embed_dim)\n",
    "        self.scaling = config.embed_dim ** 0.5 # for normalizing the first embedding\n",
    "        \n",
    "        # Initialize a sequence of DecoderLayer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_hidden_layers))\n",
    "\n",
    "        # Initialize a normalization layer to be applied after the last decoder layer, stabilizing the output\n",
    "        self.final_norm = RMSNorm(config.embed_dim, use_scale=True)\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len) list of integer token ids\n",
    "        target_token_ids: torch.Tensor = None, # a shape (batch_size, input_seq_len) list of token ids to train on\n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "        # turn the input tokens into the first resudial state using the embedding matrix\n",
    "        x = self.embedder(input_token_ids) * self.scaling # (batch_size, input_len) & (vocab_size, embed_dim) -> (batch_size, input_len, embed_dim)\n",
    "\n",
    "        # Iteratively process the input through each Layer\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply normalization to the output of the final decoder layer\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # grabbing the weights of the embedding matrix shape (vocab_size, hidden_dim) for use as the output layer\n",
    "        embedder_weight = self.embedder.weight\n",
    "\n",
    "        # the embedding matrix is also used as the output layer\n",
    "        logits = torch.matmul(x, embedder_weight.t()) # (batch_size, input_len, embed_dim) @ (embed_dim, vocab_size) -> (batch_size, input_len, vocab_size)\n",
    "        \n",
    "        if target_token_ids is None: # if we're not training, then we don't need to calculate loss\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are training\n",
    "            batch_size, input_len, vocab_size = logits.shape\n",
    "            # then we reshape our logits & targets before calculating cross-entropy loss\n",
    "            loss = self.criterion(logits.view(batch_size*input_len, vocab_size), \n",
    "                                  target_token_ids.reshape(batch_size*input_len))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling \n",
    "        \"\"\"\n",
    "        # Select the last element for each sequence.\n",
    "        logits = logits[:,-1,:] # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        logits.div_(temperature) # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k. both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "\n",
    "        ### calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "\n",
    "        ### calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_len: int = None, \n",
    "        temperature: float = 1.0, # defaulting to 1.0 means we essentially don't use temperature\n",
    "        top_p: float = 1.0, # defaulting to 1.0 means we essentially don't use top-p\n",
    "        top_k: int = config.vocab_size, # setting top_k = vocab_size means we're effectively not using top_k at all\n",
    "    ) -> str: \n",
    "        \"\"\" Wrapper around sampler() that deals with manipulation of the sequence \"\"\"\n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        \n",
    "        if output_len is None:\n",
    "            output_len = config.max_seq_len - len(tokens)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        assert len(tokens) + output_len <= self.config.max_seq_len\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len])\n",
    "            \n",
    "            next_token = self.Sampler(\n",
    "                logits = logits,\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                top_k = top_k\n",
    "            )\n",
    "            if next_token == config.vocab_size: break\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a1f17-5a2e-4bbe-bc99-e9412d2b81de",
   "metadata": {},
   "source": [
    "# Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc248f3-4a49-4f49-967f-577f06df16a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 512]),\n",
       " tensor([[ 36,  61,  50,  ...,  50,  58,  75],\n",
       "         [ 36,  61,  50,  ..., 128, 128, 128],\n",
       "         [ 36,  61,  50,  ..., 128, 128, 128],\n",
       "         ...,\n",
       "         [ 33, 113,  72,  ...,   1,  58,  86],\n",
       "         [ 36,  61,  50,  ...,  60, 113,  78],\n",
       "         [ 36,  61,  50,  ..., 128, 128, 128]]),\n",
       " torch.Size([32, 512]),\n",
       " tensor([[ 61,  50,  52,  ...,  58,  75, 110],\n",
       "         [ 61,  50,  52,  ..., 128, 128, 128],\n",
       "         [ 61,  50,  52,  ..., 128, 128, 128],\n",
       "         ...,\n",
       "         [113,  72,   1,  ...,  58,  86,  52],\n",
       "         [ 61,  50,  52,  ..., 113,  78,   1],\n",
       "         [ 61,  50,  52,  ..., 128, 128, 128]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_batch(batch, tokenizer, config):\n",
    "    \"\"\"\n",
    "    Process a batch of strings for transformer training.\n",
    "\n",
    "    Args:\n",
    "    batch (list of str): The batch of strings to process.\n",
    "    tokenizer: The tokenizer to use for encoding the strings.\n",
    "    config: Configuration object with max_seq_len and vocab_size attributes.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, torch.Tensor: The input and target tensors for the transformer.\n",
    "    \"\"\"\n",
    "    tokenized_batch = [tokenizer.encode(string)[:config.max_seq_len+1] for string in batch]\n",
    "    max_length = min(max(len(tokens) for tokens in tokenized_batch), config.max_seq_len)\n",
    "\n",
    "    # Pad the sequences\n",
    "    padded_batch = [tokens + [config.vocab_size] * (max_length+1 - len(tokens)) for tokens in tokenized_batch]\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    tensor_batch = torch.tensor(padded_batch, dtype=torch.long)\n",
    "\n",
    "    # Split into input and target tensors\n",
    "    input_tensor = tensor_batch[:, :-1]  # Exclude the last token for input\n",
    "    target_tensor = tensor_batch[:, 1:]  # Exclude the first token for target\n",
    "\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have a tokenizer and config object defined, you would call this function like this:\n",
    "x,y = process_batch(next(train_loader_iter), tokenizer, config)\n",
    "x.shape,x, y.shape,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters = 5): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = process_batch(next(val_loader_iter), tokenizer, config)\n",
    "            logits, loss = model(X.to(config.device), Y.to(config.device))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973.568 K parameters\n",
      "customGPT(\n",
      "  (embedder): Embedding(129, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (mqa): MQA(\n",
      "        (qkv_proj): Linear(in_features=128, out_features=192, bias=False)\n",
      "        (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (gate_proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (up_proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (down_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (pre_mqa_norm): RMSNorm()\n",
      "      (post_mqa_norm): RMSNorm()\n",
      "      (pre_mlp_norm): RMSNorm()\n",
      "      (post_mlp_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (final_norm): RMSNorm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = customGPT(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5242a7-6035-4f53-9d26-0690aea809d9",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d5b32a-8fc1-4fa2-a301-85142c442bd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/?.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/?.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the saved state dictionary\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print the number of parameters in the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/?.pth'"
     ]
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = customGPT(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/?.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 10\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 2\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 109.6178, val loss 111.5728, time elapsed: 2.84 seconds\n",
      "step 2: train loss 107.0684, val loss 106.8237, time elapsed: 13.12 seconds\n",
      "step 4: train loss 102.9732, val loss 101.7442, time elapsed: 23.21 seconds\n",
      "step 6: train loss 98.2700, val loss 99.1256, time elapsed: 33.25 seconds\n",
      "step 8: train loss 96.0997, val loss 95.7466, time elapsed: 42.63 seconds\n",
      "step 9: train loss 95.0671, val loss 95.4530, time elapsed: 50.42 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = process_batch(next(train_loader_iter), tokenizer, config)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb.to(config.device), yb.to(config.device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "torch.save(model.state_dict(),\n",
    "           f'models/{model.__class__.__name__}'\n",
    "           f'-vocab_size{config.vocab_size}'\n",
    "           f'-max_seq_len{config.max_seq_len}'\n",
    "           f'-num_hidden_layers{config.num_hidden_layers}'\n",
    "           f'-num_q_heads{config.num_q_heads}'\n",
    "           f'-num_kv_heads{config.num_kv_heads}'\n",
    "           f'-embed_dim{config.embed_dim}'\n",
    "           f'-mlp_multiplier{config.mlp_multiplier}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-theta{config.theta}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30e834-56f9-4812-ab4c-03e2f14874ca",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2260c4f-40b2-4b8e-9db4-ad1717a3027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou ingo\"Ukkk\n",
      "ldayVBHeddaysasa6AAAAKTK88$R888Lchishersa2eouededT9airir\"O-otRon$Piot. theaaoG1irouSTheOOG1nejUandNpjjSwassbOothexjtheCJJirNhere?veralliHe k0BBZwzRimasasawabehishisallriverin. ut3the. theplrxararthe\"HeHeHeZZVloandand::ppc6\n",
      "\n",
      "wasar1hisjededMut;1erstwasQor\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\n",
    "max_useable_output_len = config.max_seq_len - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len, temperature=50.0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f258187-bf1d-482c-9c96-9cf21be21daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c0a4474-2503-452b-8054-e5d129557c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode('JULIET:\\nO Romeo, Romeo! wherefore art thou 9b\\nerAk!ot11EEerr$L$$$toplayplayplay2dwSou1waswas. sdha9ll,wasulTheJheomJJ, zplayplayTheyplayver?zEer;NP5000baThey?playplayplaver, stonmm4allCiPPha2CenenhiswawasirXandiMor,A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7ef80-36ef-4153-a1f2-de17fa396ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
