{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb200416-d927-4302-b6b2-a0bfe159c321",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, './venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209f309f-df73-4183-9d1c-a4fb834e825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=64, vocab_len=2051, device='cpu', num_layers=4, second_resid_norm=False, mlp_hidden_mult=4, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=16, num_kv_heads=4, head_dim=16, theta=10000, max_seq_len=256, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1) \n",
      "\n",
      " TrainConfig(weight_decay=0.02, batch_size=32, max_iters=5, eval_interval=2, eval_samples=1, checkpoint_interval=None, lr_max=0.1, lr_min=1e-05, warmup_iters=0, final_flat_iters=0, anneal_type='cos', num_restarts=3, T_mult=2)\n",
      "492.864 K parameters\n",
      "\n",
      "customGPT(\n",
      "  (token_embedder): Embedding(2051, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x ResidualLayer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQSA(\n",
      "        (Wq): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wk): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (Wv): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (Wo): Linear(in_features=256, out_features=64, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wup): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=64, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "from tokenizer import get_tokenizer\n",
    "tokenizer = get_tokenizer(size = 2048) # size options are 95(character-wise), 128, 256, 512, 1024, 2048 & 4096\n",
    "\n",
    "# config file\n",
    "from config import ModelConfig, TrainConfig\n",
    "cfg = ModelConfig()\n",
    "cfg.vocab_len = tokenizer.vocab_len\n",
    "tcfg = TrainConfig()\n",
    "print(cfg, '\\n\\n', tcfg)\n",
    "\n",
    "# model modules\n",
    "from model import customGPT\n",
    "model = customGPT(cfg).to(cfg.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters\\n')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6926710-6418-47c1-ba9f-ac9d63c092d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/tunadorable/.cache/huggingface/datasets/noanabeshima___json/noanabeshima--TinyStoriesV2-226173b7dd235c68/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Found cached dataset json (/Users/tunadorable/.cache/huggingface/datasets/noanabeshima___json/noanabeshima--TinyStoriesV2-226173b7dd235c68/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tools import get_data_loader\n",
    "from train import scheduler_lambda, train\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = tcfg.lr_max, weight_decay = tcfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler_lambda)\n",
    "\n",
    "train_data_loader = get_data_loader(batch_size=tcfg.batch_size, split='train')\n",
    "test_data_loader = get_data_loader(batch_size=tcfg.batch_size, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85b7902-ee37-4963-b263-f28e72ecaf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # set to true if you'd like to see a graph of the learning rate schedule\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Generate learning rate values\n",
    "    lrs = [scheduler_lambda(i) for i in range(tcfg.max_iters)]\n",
    "    \n",
    "    # Plot the learning rates\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(lrs, label='Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████                        | 1/5 [00:02<00:08,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0000: lr 0.010000, train loss 59.7845, val loss 59.6585, ppl 81163229730732260594810880, time elapsed: 1.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████            | 3/5 [00:04<00:03,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0002: lr 0.001465, train loss 45.5886, val loss 44.9978, ppl 34857197010824462336, time elapsed: 3.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 5/5 [00:07<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0004: lr 0.003087, train loss 23.0972, val loss 23.1613, ppl 11450800128, time elapsed: 6.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, log_data = train(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    cfg, \n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    tcfg, \n",
    "    train_data_loader,\n",
    "    test_data_loader,\n",
    "    #log_data: list = None, \n",
    "    #detect_anomoly = False # use if you're getting crazy errors about a the gradient being broken\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc1adb-a1fd-4f0e-8666-26b2a99a0e54",
   "metadata": {},
   "source": [
    "# inference test before you decide to save it\n",
    "if `tcfg.checkpoint_interval != None` then checkpoints have already been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "565eeefb-ce04-44c5-a0b7-81a1a75786c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeuse use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use use \n"
     ]
    }
   ],
   "source": [
    "from inference import generate\n",
    "prompt = \"Once upon a time\"\n",
    "model.eval()\n",
    "output = generate(\n",
    "    prompt, \n",
    "    model, \n",
    "    tokenizer,\n",
    "    #max_gen_len = 512,\n",
    "    temperature = 0.7,\n",
    "    #memory_saver_div = 8,\n",
    "    #top_p = 0.9,\n",
    "    #top_k = 32,\n",
    ")\n",
    "model.train()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your final model\n",
    "you DO still need to do this even if you had been saving checkpoints; the final state has not yet been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import save_model\n",
    "save_model(model, cfg, tcfg, log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf7ada3-6ad3-49de-9664-a7295d6085c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
