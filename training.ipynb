{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb200416-d927-4302-b6b2-a0bfe159c321",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, './venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# model modules\n",
    "from model import *\n",
    "\n",
    "# used in the training loop\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# used to save & load models\n",
    "import json\n",
    "from dataclasses import asdict\n",
    "\n",
    "# inference code\n",
    "from inference import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209f309f-df73-4183-9d1c-a4fb834e825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(dim=128, vocab_len=515, device='cpu', num_layers=10, pre_connect_dropout=False, second_resid_norm=False, mlp_hidden_mult=2, mlp_bias=False, mlp_nonlinearity='GeLU', mlp_gated=True, num_q_heads=4, num_kv_heads=1, theta=10000, max_seq_len=512, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1)\n"
     ]
    }
   ],
   "source": [
    "# tokenizer\n",
    "from tokenizer import *\n",
    "size = 512 # size options are 128, 256, 512 and 1024\n",
    "path = f'./tokenizers/tiny_stories_tokenizer_{size}.model'\n",
    "tokenizer = get_tokenizer(path) \n",
    "\n",
    "# config file\n",
    "from config import *\n",
    "cfg = Config()\n",
    "cfg.vocab_len = tokenizer.vocab_len\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463.936 K parameters\n",
      "customGPT(\n",
      "  (token_embedder): Embedding(515, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-9): 10 x ResidualLayer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQSA(\n",
      "        (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (Wv): Linear(in_features=128, out_features=32, bias=False)\n",
      "        (Wo): Linear(in_features=128, out_features=128, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wgate): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (Wup): Linear(in_features=128, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=128, bias=False)\n",
      "        (nonlinearity): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = customGPT(cfg).to(cfg.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7783674b-d5ea-4e45-9299-477ced929d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        # Load the dataset\n",
    "        self.dataset = load_dataset(\"noanabeshima/TinyStoriesV2\", split=split)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch one item from the dataset\n",
    "        return self.dataset[idx]['text']\n",
    "\n",
    "def get_data_loader(batch_size=32, shuffle=True, split='train', num_workers=0):\n",
    "    # Create the dataset\n",
    "    dataset = TinyStoriesDataset(split)\n",
    "    # Create the DataLoader\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75002107-a416-4b08-b42c-515b6dca220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/tunadorable/.cache/huggingface/datasets/noanabeshima___json/noanabeshima--TinyStoriesV2-226173b7dd235c68/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Found cached dataset json (/Users/tunadorable/.cache/huggingface/datasets/noanabeshima___json/noanabeshima--TinyStoriesV2-226173b7dd235c68/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_data_loader = get_data_loader(batch_size=batch_size, split='train')\n",
    "test_data_loader = get_data_loader(batch_size=batch_size, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d1345b-052c-4977-9192-6b3ab0e70881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was once a woman named Alexandra who was very charming. One day, Alexandra stepped out of her house to go for a walk. As she was walking, she heard a voice calling her name. It was her neighbour, Peter. Peter was also very charming. He asked her what she was doing and Alexandra said she was taking a stroll. He then offered to join her.\n",
      "They walked together and started chatting. Alexandra was feeling a little peckish, so Peter offered her some ice cream. She happily accepted, and it was the most delicious ice cream she had ever tasted. They shared their ice creams and enjoyed the beautiful sunshine.\n",
      "When it was time to go home, Alexandra thanked Peter for the delicious cream and for the lovely walk. \"It was so much fun having you as my companion,\" she said. Peter blushed upon hearing her kind words. Before saying goodbye, Peter stepped forward and gave her a big hug.\n",
      "Alexandra walked home feeling very content.\n"
     ]
    }
   ],
   "source": [
    "# To get a batch of data\n",
    "b = next(iter(train_data_loader))\n",
    "print(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torcherize_batch(batch, max_seq_len):\n",
    "    b = torch.zeros(len(batch), max_seq_len+1)\n",
    "    for i, s in enumerate(batch):\n",
    "        b[i] = torch.tensor(\n",
    "            tokenizer.encode(s, bos=True, eos=True, pad=max_seq_len+1), \n",
    "            device=cfg.device\n",
    "        )\n",
    "    x, y = b[:,:max_seq_len], b[:, 1:]\n",
    "    return x.to(torch.long), y.to(torch.long)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, dataloader, eval_iters = 3): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            batch = next(iter(dataloader))\n",
    "            X, Y = torcherize_batch(batch, model.max_seq_len)\n",
    "            logits, loss = model(X, target_token_ids=Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "lr_init = 1e-2\n",
    "weight_decay = 0.02\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr_init, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 4\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 2\n",
    "\n",
    "# Warmup setup\n",
    "warmup_iters = 2  # Number of warmup iterations\n",
    "warmup_factor = 1e-3  # Warmup factor (initial learning rate is multiplied by this factor)\n",
    "\n",
    "lr_final = 1e-5  # Minimum learning rate\n",
    "\n",
    "def lr_lambda(current_iter):\n",
    "    if current_iter < warmup_iters:\n",
    "        # Warmup phase\n",
    "        return warmup_factor + (1 - warmup_factor) * current_iter / warmup_iters\n",
    "    else:\n",
    "        # Cosine decay phase with minimum learning rate\n",
    "        decay_iters = max_iters - warmup_iters\n",
    "        cosine_decay = 0.5 * (1 + math.cos(math.pi * (current_iter - warmup_iters) / decay_iters))\n",
    "        return max(cosine_decay, lr_final / lr_init)\n",
    "        \n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0000: lr 0.005005, train loss 102.0891, val loss 103.0794, time elapsed: 4.22 seconds\n",
      "step 0002: lr 0.005000, train loss 50.0940, val loss 48.9597, time elapsed: 23.66 seconds\n",
      "step 0003: lr 0.000010, train loss 37.1550, val loss 35.8669, time elapsed: 38.88 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for i in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    batch = next(iter(train_data_loader))\n",
    "    x,y = torcherize_batch(batch, cfg.max_seq_len)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(x, target_token_ids=y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if i % eval_interval == 0 or i == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, test_data_loader)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(\n",
    "            f\"step {i:04d}: \"\n",
    "            f\"lr {current_lr:.6f}, \"\n",
    "            f\"train loss {losses['train'].item():.4f}, \"\n",
    "            f\"val loss {losses['val'].item():.4f}, \"\n",
    "            f\"time elapsed: {elapsed_time:.2f} seconds\"\n",
    "        )\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc1adb-a1fd-4f0e-8666-26b2a99a0e54",
   "metadata": {},
   "source": [
    "# inference test before you decide to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "565eeefb-ce04-44c5-a0b7-81a1a75786c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum attention matrix size in memory will be 128x512 rather than 512x512\n",
      "\n",
      "Once upon a time, there was a boy named Evin                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time, there was a boy named Evin\"\n",
    "output = generate(\n",
    "    prompt, \n",
    "    model, \n",
    "    tokenizer,\n",
    "    memory_saver_div = 4,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'models/{model.__class__.__name__}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}'\n",
    "torch.save(model.state_dict(), f'{name}.pth')\n",
    "\n",
    "# Convert the dataclass object to a dictionary\n",
    "cfg_dict = asdict(cfg)\n",
    "\n",
    "# Serialize the dictionary to a JSON file\n",
    "with open(f'{name}.json', 'w') as f:\n",
    "    json.dump(cfg_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140c2e2-02fe-4e6c-b745-7d26957e4c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
