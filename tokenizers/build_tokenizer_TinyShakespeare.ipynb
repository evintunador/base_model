{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80438c07-ff56-4f17-bd89-53ca3f855407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# load the TinyShakespare dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b44aae-e141-43d7-97fb-24bdf4eeb124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "length: 200\n",
      "---\n",
      "[70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 58, 10, 66, 101, 102, 111, 114, 101, 32, 119, 101, 32, 112, 114, 111, 99, 101, 101, 100, 32, 97, 110, 121, 32, 102, 117, 114, 116, 104, 101, 114, 44, 32, 104, 101, 97, 114, 32, 109, 101, 32, 115, 112, 101, 97, 107, 46, 10, 10, 65, 108, 108, 58, 10, 83, 112, 101, 97, 107, 44, 32, 115, 112, 101, 97, 107, 46, 10, 10, 70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 58, 10, 89, 111, 117, 32, 97, 114, 101, 32, 97, 108, 108, 32, 114, 101, 115, 111, 108, 118, 101, 100, 32, 114, 97, 116, 104, 101, 114, 32, 116, 111, 32, 100, 105, 101, 32, 116, 104, 97, 110, 32, 116, 111, 32, 102, 97, 109, 105, 115, 104, 63, 10, 10, 65, 108, 108, 58, 10, 82, 101, 115, 111, 108, 118, 101, 100, 46, 32, 114, 101, 115, 111, 108, 118, 101, 100, 46, 10, 10, 70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 58, 10, 70, 105, 114, 115, 116, 44, 32, 121, 111, 117]\n",
      "length: 200\n"
     ]
    }
   ],
   "source": [
    "tokens = text[:200].encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text[:200])\n",
    "print(\"length:\", len(text[:200]))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e6992-bb94-4537-8d4e-7a95fdc310ed",
   "metadata": {},
   "source": [
    "We happen to get the same length here because we're using such simple/common characters. If we used foreign characters or emoji then they'd each take up more than one byte. But that won't really be an issue with TinyShakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1f1e7ef-8507-43d4-a4d7-313a065e4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "length: 533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 616\n"
     ]
    }
   ],
   "source": [
    "# just to prove what i mean\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "example_text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = example_text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(example_text)\n",
    "print(\"length:\", len(example_text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4996c7f2-5078-4c53-ae3a-3c6905168a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, (101, 32)), (5, (58, 10)), (4, (115, 116)), (4, (114, 115)), (4, (114, 101)), (4, (105, 114)), (4, (101, 100)), (4, (101, 97)), (4, (70, 105)), (4, (10, 10)), (3, (122, 101)), (3, (118, 101)), (3, (116, 105)), (3, (116, 104)), (3, (116, 32)), (3, (115, 111)), (3, (112, 101)), (3, (111, 108)), (3, (110, 58)), (3, (108, 118)), (3, (108, 108)), (3, (105, 122)), (3, (105, 116)), (3, (104, 101)), (3, (101, 115)), (3, (101, 110)), (3, (97, 107)), (3, (67, 105)), (3, (46, 10)), (3, (44, 32)), (3, (32, 116)), (3, (32, 114)), (3, (32, 97)), (3, (32, 67)), (3, (10, 70)), (2, (116, 111)), (2, (115, 112)), (2, (114, 32)), (2, (111, 117)), (2, (111, 32)), (2, (108, 58)), (2, (107, 46)), (2, (101, 114)), (2, (100, 46)), (2, (100, 32)), (2, (97, 114)), (2, (97, 110)), (2, (65, 108)), (2, (32, 115)), (2, (32, 102)), (2, (10, 65)), (1, (121, 111)), (1, (121, 32)), (1, (119, 101)), (1, (117, 114)), (1, (117, 32)), (1, (116, 44)), (1, (115, 104)), (1, (114, 116)), (1, (114, 111)), (1, (114, 97)), (1, (114, 44)), (1, (112, 114)), (1, (111, 114)), (1, (111, 99)), (1, (110, 121)), (1, (110, 32)), (1, (109, 105)), (1, (109, 101)), (1, (108, 32)), (1, (107, 44)), (1, (105, 115)), (1, (105, 101)), (1, (104, 97)), (1, (104, 63)), (1, (102, 117)), (1, (102, 111)), (1, (102, 97)), (1, (101, 102)), (1, (101, 101)), (1, (100, 105)), (1, (99, 101)), (1, (97, 116)), (1, (97, 109)), (1, (97, 108)), (1, (89, 111)), (1, (83, 112)), (1, (82, 101)), (1, (66, 101)), (1, (63, 10)), (1, (46, 32)), (1, (32, 121)), (1, (32, 119)), (1, (32, 112)), (1, (32, 109)), (1, (32, 104)), (1, (32, 100)), (1, (10, 89)), (1, (10, 83)), (1, (10, 82)), (1, (10, 66))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "# let's only do the first 200 characters for now for demonstration purposes\n",
    "tokens = text[:200].encode(\"utf-8\")\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "\n",
    "#print(stats)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse=True))\n",
    "# so these are all the pairs of tokens in the text found in order of how often they show up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4872443-48a4-4290-9b25-281e5ac0d77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this was the most common pair\n",
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552d7eae-4835-4f93-b7a0-5315ef1dd472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n",
      "[70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 256, 66, 101, 102, 111, 114, 101, 32, 119, 101, 32, 112, 114, 111, 99, 101, 101, 100, 32, 97, 110, 121, 32, 102, 117, 114, 116, 104, 101, 114, 44, 32, 104, 101, 97, 114, 32, 109, 101, 32, 115, 112, 101, 97, 107, 46, 10, 10, 65, 108, 108, 256, 83, 112, 101, 97, 107, 44, 32, 115, 112, 101, 97, 107, 46, 10, 10, 70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 256, 89, 111, 117, 32, 97, 114, 101, 32, 97, 108, 108, 32, 114, 101, 115, 111, 108, 118, 101, 100, 32, 114, 97, 116, 104, 101, 114, 32, 116, 111, 32, 100, 105, 101, 32, 116, 104, 97, 110, 32, 116, 111, 32, 102, 97, 109, 105, 115, 104, 63, 10, 10, 65, 108, 108, 256, 82, 101, 115, 111, 108, 118, 101, 100, 46, 32, 114, 101, 115, 111, 108, 118, 101, 100, 46, 10, 10, 70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 256, 70, 105, 114, 115, 116, 44, 32, 121, 111, 117]\n",
      "length: 195\n"
     ]
    }
   ],
   "source": [
    "# so this function will merge a single pair for us\n",
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# an example of how it works\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "\n",
    "# let's do it with our actual top pair\n",
    "tokens2 = merge(tokens, top_pair, 256) # 256 is the id of our new token\n",
    "print(tokens2)\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "120da24f-07f3-4298-8e97-9ac546a63a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (58, 10) into a new token 256\n",
      "merging (101, 32) into a new token 257\n",
      "merging (70, 105) into a new token 258\n",
      "merging (258, 114) into a new token 259\n",
      "merging (259, 115) into a new token 260\n",
      "merging (260, 116) into a new token 261\n",
      "merging (101, 100) into a new token 262\n",
      "merging (101, 97) into a new token 263\n",
      "merging (10, 10) into a new token 264\n",
      "merging (261, 32) into a new token 265\n",
      "merging (265, 67) into a new token 266\n",
      "merging (266, 105) into a new token 267\n",
      "merging (267, 116) into a new token 268\n",
      "merging (268, 105) into a new token 269\n",
      "merging (269, 122) into a new token 270\n",
      "merging (270, 101) into a new token 271\n",
      "merging (271, 110) into a new token 272\n",
      "merging (272, 256) into a new token 273\n",
      "merging (116, 104) into a new token 274\n",
      "merging (44, 32) into a new token 275\n",
      "merging (112, 263) into a new token 276\n",
      "merging (276, 107) into a new token 277\n",
      "merging (46, 264) into a new token 278\n",
      "merging (108, 108) into a new token 279\n",
      "merging (32, 114) into a new token 280\n",
      "merging (101, 115) into a new token 281\n",
      "merging (281, 111) into a new token 282\n",
      "merging (282, 108) into a new token 283\n",
      "merging (283, 118) into a new token 284\n",
      "merging (284, 262) into a new token 285\n",
      "merging (114, 257) into a new token 286\n",
      "merging (32, 97) into a new token 287\n",
      "merging (32, 102) into a new token 288\n",
      "merging (274, 101) into a new token 289\n",
      "merging (289, 114) into a new token 290\n",
      "merging (115, 277) into a new token 291\n",
      "merging (291, 278) into a new token 292\n",
      "merging (65, 279) into a new token 293\n",
      "merging (293, 256) into a new token 294\n",
      "merging (111, 117) into a new token 295\n",
      "merging (280, 285) into a new token 296\n",
      "merging (32, 116) into a new token 297\n",
      "merging (297, 111) into a new token 298\n",
      "merging (273, 66) into a new token 299\n",
      "merging (299, 101) into a new token 300\n",
      "merging (300, 102) into a new token 301\n",
      "merging (301, 111) into a new token 302\n",
      "merging (302, 286) into a new token 303\n",
      "merging (303, 119) into a new token 304\n",
      "merging (304, 257) into a new token 305\n",
      "merging (305, 112) into a new token 306\n",
      "merging (306, 114) into a new token 307\n",
      "merging (307, 111) into a new token 308\n",
      "merging (308, 99) into a new token 309\n",
      "merging (309, 101) into a new token 310\n",
      "merging (310, 262) into a new token 311\n",
      "merging (311, 287) into a new token 312\n",
      "merging (312, 110) into a new token 313\n",
      "merging (313, 121) into a new token 314\n",
      "merging (314, 288) into a new token 315\n",
      "merging (315, 117) into a new token 316\n",
      "merging (316, 114) into a new token 317\n",
      "merging (317, 290) into a new token 318\n",
      "merging (318, 275) into a new token 319\n",
      "merging (319, 104) into a new token 320\n",
      "merging (320, 263) into a new token 321\n",
      "merging (321, 114) into a new token 322\n",
      "merging (322, 32) into a new token 323\n",
      "merging (323, 109) into a new token 324\n",
      "merging (324, 257) into a new token 325\n",
      "merging (325, 292) into a new token 326\n",
      "merging (326, 294) into a new token 327\n",
      "merging (327, 83) into a new token 328\n",
      "merging (328, 277) into a new token 329\n",
      "merging (329, 275) into a new token 330\n",
      "merging (330, 292) into a new token 331\n",
      "merging (331, 273) into a new token 332\n",
      "merging (332, 89) into a new token 333\n",
      "merging (333, 295) into a new token 334\n",
      "merging (334, 287) into a new token 335\n",
      "merging (335, 286) into a new token 336\n",
      "merging (336, 97) into a new token 337\n",
      "merging (337, 279) into a new token 338\n",
      "merging (338, 296) into a new token 339\n",
      "merging (339, 280) into a new token 340\n",
      "merging (340, 97) into a new token 341\n",
      "merging (341, 290) into a new token 342\n",
      "merging (342, 298) into a new token 343\n",
      "merging (343, 32) into a new token 344\n",
      "merging (344, 100) into a new token 345\n",
      "merging (345, 105) into a new token 346\n",
      "merging (346, 257) into a new token 347\n",
      "merging (347, 274) into a new token 348\n",
      "merging (348, 97) into a new token 349\n",
      "merging (349, 110) into a new token 350\n",
      "merging (350, 298) into a new token 351\n",
      "merging (351, 288) into a new token 352\n",
      "merging (352, 97) into a new token 353\n",
      "merging (353, 109) into a new token 354\n",
      "merging (354, 105) into a new token 355\n",
      "merging (355, 115) into a new token 356\n",
      "merging (356, 104) into a new token 357\n",
      "merging (357, 63) into a new token 358\n",
      "merging (358, 264) into a new token 359\n",
      "merging (359, 294) into a new token 360\n",
      "merging (360, 82) into a new token 361\n",
      "merging (361, 285) into a new token 362\n",
      "merging (362, 46) into a new token 363\n",
      "merging (363, 296) into a new token 364\n",
      "merging (364, 278) into a new token 365\n",
      "merging (365, 273) into a new token 366\n",
      "merging (366, 261) into a new token 367\n",
      "merging (367, 275) into a new token 368\n",
      "merging (368, 121) into a new token 369\n",
      "merging (369, 295) into a new token 370\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_merges):\n\u001b[1;32m      8\u001b[0m   stats \u001b[38;5;241m=\u001b[39m get_stats(ids)\n\u001b[0;32m----> 9\u001b[0m   pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(stats, key\u001b[38;5;241m=\u001b[39mstats\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m     10\u001b[0m   idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m+\u001b[39m i\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerging \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into a new token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "vocab_size = 1024 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "# now let's actually do it\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2596a6b1-f4f8-4051-9012-bc9ed0a08511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 200\n",
      "ids length: 72\n",
      "compression ratio: 2.78X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens)) # remember tokens are our original tokens\n",
    "print(\"ids length:\", len(ids)) # and ids are new tokens we've made\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6402ac6d-c082-45b0-8275-be6d6bc6c477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a1e67ea4-1aaa-45ed-88ef-89f723fa2aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 279, 111, 32, 87, 111, 114, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "  # given a string, return list of integers (the tokens)\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2:\n",
    "    stats = get_stats(tokens)\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"Hello World!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b94abe4b-7a3c-4916-8e84-b6a671afa3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))\n",
    "\n",
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc33c2ae-caf2-4c1b-8f7b-091ea4c2c8dd",
   "metadata": {},
   "source": [
    "now instead of just byte-pair encoding, we're also going to enforce what are called \"regex\" rules that basically prevent the tokenizer from combining certain bytes. For example, we humans think that special characters like `!` are notably different from letters, so we don't want the tokenizer to combine `n` and `!` into one token `n!`. It also does other things like merge long sequences of spaces or newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "285587d8-14e7-4dd9-9cc5-dccccb4937ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cde6c08a-35bd-445d-b0ac-572b9e2d1bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\")) # 220 is the \" \" token\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daace11-8797-47c3-a47f-d1ba7c78284c",
   "metadata": {},
   "source": [
    "Personally tho I have no interest in doing all that the way they did given my tiny dataset. I just want a tokenizer with more than 65 characters to give my tiny test model a more realistic modeling experience, and tinyShakespeare doesn't have a whole lot of special characters anyways. So we're gonna enforce an over-simplified version of regex which takes advantage of the fact that the first 13 characters are non-alphabetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b5685-ef9d-4867-a929-80764307efd7",
   "metadata": {},
   "source": [
    "# Actually Building It\n",
    "\n",
    "We're gonna make it hella simple, start with our 65 unique characters, and turn them into 128 total tokens that are either composed of letters or non-letters but not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6484b487-7334-4404-8260-03c8ce907b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231c1fb5-c90b-4bac-9a65-cd6bcfcbef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "char_encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "\n",
    "tokens = char_encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4a32dbf-882c-4e62-8be3-eaf03d5fb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 256#128 # the desired final vocabulary size\n",
    "num_merges = vocab_size - v\n",
    "ids = list(tokens) # copy so we don't destroy the original list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8d5c466-ae0c-4723-9922-fdde1233e268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]\n",
      "['symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter']\n"
     ]
    }
   ],
   "source": [
    "base_indices = char_encode(chars)\n",
    "print(base_indices)\n",
    "origin = [ \"symbol\" if i < 13 else \"letter\" for i in base_indices]  # Track token origin\n",
    "print(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3f909a1-c258-44c1-a102-961f6eca66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27c33cc7-78b9-464b-8089-8c594c672e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (58, 46) into a new token 65\n",
      "merging (6, 1) into a new token 66\n",
      "merging (53, 59) into a new token 67\n",
      "merging (43, 56) into a new token 68\n",
      "merging (47, 52) into a new token 69\n",
      "merging (39, 52) into a new token 70\n",
      "merging (10, 0) into a new token 71\n",
      "merging (65, 43) into a new token 72\n",
      "merging (53, 56) into a new token 73\n",
      "merging (47, 57) into a new token 74\n",
      "merging (0, 0) into a new token 75\n",
      "merging (43, 52) into a new token 76\n",
      "merging (39, 56) into a new token 77\n",
      "merging (39, 58) into a new token 78\n",
      "merging (53, 52) into a new token 79\n",
      "merging (57, 58) into a new token 80\n",
      "merging (50, 50) into a new token 81\n",
      "merging (6, 0) into a new token 82\n",
      "merging (51, 43) into a new token 83\n",
      "merging (58, 53) into a new token 84\n",
      "merging (8, 75) into a new token 85\n",
      "merging (70, 42) into a new token 86\n",
      "merging (46, 43) into a new token 87\n",
      "merging (63, 67) into a new token 88\n",
      "merging (43, 57) into a new token 89\n",
      "merging (52, 53) into a new token 90\n",
      "merging (57, 43) into a new token 91\n",
      "merging (46, 39) into a new token 92\n",
      "merging (56, 43) into a new token 93\n",
      "merging (53, 44) into a new token 94\n",
      "merging (60, 43) into a new token 95\n",
      "merging (47, 58) into a new token 96\n",
      "merging (69, 45) into a new token 97\n",
      "merging (40, 43) into a new token 98\n",
      "merging (50, 43) into a new token 99\n",
      "merging (61, 47) into a new token 100\n",
      "merging (51, 63) into a new token 101\n",
      "merging (46, 47) into a new token 102\n",
      "merging (53, 61) into a new token 103\n",
      "merging (41, 43) into a new token 104\n",
      "merging (44, 73) into a new token 105\n",
      "merging (39, 63) into a new token 106\n",
      "merging (39, 57) into a new token 107\n",
      "merging (41, 46) into a new token 108\n",
      "merging (52, 42) into a new token 109\n",
      "merging (68, 43) into a new token 110\n",
      "merging (50, 42) into a new token 111\n",
      "merging (47, 56) into a new token 112\n",
      "merging (43, 42) into a new token 113\n",
      "merging (59, 58) into a new token 114\n",
      "merging (56, 53) into a new token 115\n",
      "merging (90, 58) into a new token 116\n",
      "merging (50, 47) into a new token 117\n",
      "merging (42, 43) into a new token 118\n",
      "merging (61, 43) into a new token 119\n",
      "merging (46, 78) into a new token 120\n",
      "merging (65, 78) into a new token 121\n",
      "merging (45, 46) into a new token 122\n",
      "merging (13, 109) into a new token 123\n",
      "merging (49, 43) into a new token 124\n",
      "merging (11, 1) into a new token 125\n",
      "merging (53, 53) into a new token 126\n",
      "merging (67, 56) into a new token 127\n",
      "merging (100, 65) into a new token 128\n",
      "merging (53, 83) into a new token 129\n",
      "merging (11, 0) into a new token 130\n",
      "merging (76, 58) into a new token 131\n",
      "merging (46, 74) into a new token 132\n",
      "merging (8, 0) into a new token 133\n",
      "merging (56, 47) into a new token 134\n",
      "merging (88, 56) into a new token 135\n",
      "merging (65, 67) into a new token 136\n",
      "merging (10, 1) into a new token 137\n",
      "merging (65, 68) into a new token 138\n",
      "merging (39, 42) into a new token 139\n",
      "merging (39, 50) into a new token 140\n",
      "merging (43, 58) into a new token 141\n",
      "merging (39, 81) into a new token 142\n",
      "merging (122, 58) into a new token 143\n",
      "merging (17, 26) into a new token 144\n",
      "merging (59, 56) into a new token 145\n",
      "merging (59, 57) into a new token 146\n",
      "merging (92, 95) into a new token 147\n",
      "merging (102, 51) into a new token 148\n",
      "merging (21, 26) into a new token 149\n",
      "merging (50, 63) into a new token 150\n",
      "merging (57, 53) into a new token 151\n",
      "merging (12, 75) into a new token 152\n",
      "merging (56, 39) into a new token 153\n",
      "merging (65, 74) into a new token 154\n",
      "merging (21, 27) into a new token 155\n",
      "merging (73, 42) into a new token 156\n",
      "merging (42, 53) into a new token 157\n",
      "merging (59, 52) into a new token 158\n",
      "merging (46, 53) into a new token 159\n",
      "merging (51, 39) into a new token 160\n",
      "merging (8, 1) into a new token 161\n",
      "merging (89, 57) into a new token 162\n",
      "merging (54, 43) into a new token 163\n",
      "merging (44, 43) into a new token 164\n",
      "merging (13, 30) into a new token 165\n",
      "merging (67, 111) into a new token 166\n",
      "merging (77, 43) into a new token 167\n",
      "merging (50, 53) into a new token 168\n",
      "merging (46, 110) into a new token 169\n",
      "merging (126, 42) into a new token 170\n",
      "merging (47, 42) into a new token 171\n",
      "merging (65, 63) into a new token 172\n",
      "merging (60, 68) into a new token 173\n",
      "merging (44, 39) into a new token 174\n",
      "merging (41, 49) into a new token 175\n",
      "merging (79, 43) into a new token 176\n",
      "merging (51, 70) into a new token 177\n",
      "merging (58, 47) into a new token 178\n",
      "merging (90, 61) into a new token 179\n",
      "merging (32, 87) into a new token 180\n",
      "merging (57, 92) into a new token 181\n",
      "merging (39, 51) into a new token 182\n",
      "merging (100, 81) into a new token 183\n",
      "merging (52, 43) into a new token 184\n",
      "merging (45, 43) into a new token 185\n",
      "merging (39, 69) into a new token 186\n",
      "merging (40, 114) into a new token 187\n",
      "merging (57, 59) into a new token 188\n",
      "merging (77, 42) into a new token 189\n",
      "merging (46, 68) into a new token 190\n",
      "merging (33, 31) into a new token 191\n",
      "merging (2, 1) into a new token 192\n",
      "merging (58, 68) into a new token 193\n",
      "merging (41, 79) into a new token 194\n",
      "merging (54, 56) into a new token 195\n",
      "merging (32, 46) into a new token 196\n",
      "merging (32, 53) into a new token 197\n",
      "merging (76, 42) into a new token 198\n",
      "merging (40, 63) into a new token 199\n",
      "merging (72, 43) into a new token 200\n",
      "merging (59, 43) into a new token 201\n",
      "merging (53, 51) into a new token 202\n",
      "merging (50, 39) into a new token 203\n",
      "merging (181, 81) into a new token 204\n",
      "merging (77, 58) into a new token 205\n",
      "merging (51, 73) into a new token 206\n",
      "merging (59, 54) into a new token 207\n",
      "merging (33, 15) into a new token 208\n",
      "merging (57, 46) into a new token 209\n",
      "merging (41, 129) into a new token 210\n",
      "merging (54, 53) into a new token 211\n",
      "merging (149, 19) into a new token 212\n",
      "merging (115, 51) into a new token 213\n",
      "merging (103, 52) into a new token 214\n",
      "merging (13, 26) into a new token 215\n",
      "merging (59, 80) into a new token 216\n",
      "merging (58, 39) into a new token 217\n",
      "merging (32, 120) into a new token 218\n",
      "merging (58, 56) into a new token 219\n",
      "merging (61, 120) into a new token 220\n",
      "merging (17, 30) into a new token 221\n",
      "merging (27, 30) into a new token 222\n",
      "merging (67, 57) into a new token 223\n",
      "merging (51, 53) into a new token 224\n",
      "merging (91, 50) into a new token 225\n",
      "merging (2, 0) into a new token 226\n",
      "merging (17, 32) into a new token 227\n",
      "merging (58, 43) into a new token 228\n",
      "merging (17, 31) into a new token 229\n",
      "merging (45, 53) into a new token 230\n",
      "merging (45, 170) into a new token 231\n",
      "merging (47, 44) into a new token 232\n",
      "merging (47, 81) into a new token 233\n",
      "merging (14, 114) into a new token 234\n",
      "merging (69, 43) into a new token 235\n",
      "merging (12, 1) into a new token 236\n",
      "merging (39, 65) into a new token 237\n",
      "merging (46, 76) into a new token 238\n",
      "merging (2, 75) into a new token 239\n",
      "merging (40, 99) into a new token 240\n",
      "merging (47, 143) into a new token 241\n",
      "merging (50, 156) into a new token 242\n",
      "merging (102, 108) into a new token 243\n",
      "merging (47, 79) into a new token 244\n",
      "merging (49, 97) into a new token 245\n",
      "merging (35, 120) into a new token 246\n",
      "merging (61, 107) into a new token 247\n",
      "merging (40, 53) into a new token 248\n",
      "merging (47, 50) into a new token 249\n",
      "merging (70, 58) into a new token 250\n",
      "merging (60, 76) into a new token 251\n",
      "merging (23, 212) into a new token 252\n",
      "merging (57, 87) into a new token 253\n",
      "merging (41, 39) into a new token 254\n",
      "merging (12, 0) into a new token 255\n"
     ]
    }
   ],
   "source": [
    "# now let's actually do it\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    #print(i)\n",
    "    stats = get_stats(ids)\n",
    "\n",
    "    # Modified pair selection logic:\n",
    "    while True:\n",
    "        pair = max(stats, key=stats.get)  # Get the most frequent pair initially\n",
    "        #print(pair)\n",
    "\n",
    "        if origin[pair[0]] != origin[pair[1]]: # Check if origins differ\n",
    "            #print(origin[pair[0]],origin[pair[1]],origin[pair[0]] != origin[pair[1]])\n",
    "            del stats[pair]\n",
    "        else:  # If no valid pairs left, break out of the loop\n",
    "            #print(origin[pair[0]],origin[pair[1]],origin[pair[0]] != origin[pair[1]])\n",
    "            break\n",
    "    else:\n",
    "        break  # Valid pair found \n",
    "    \n",
    "    pair = max(stats, key=stats.get)\n",
    "    #print(pair)\n",
    "\n",
    "    idx = v + i\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "    origin.append(origin[pair[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5ffdf00-81f1-4ec0-ab00-a688faa75d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 1115394\n",
      "ids length: 687368\n",
      "compression ratio: 1.62X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens)) # remember tokens are our original tokens\n",
    "print(\"ids length:\", len(ids)) # and ids are new tokens we've made\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc330c-e67e-4669-8556-8d53c8e843ea",
   "metadata": {},
   "source": [
    "This is pretty ideal because if we did too large of a compression ratio we'd be making our dataset size way too small to be useable. notice how if we hadn't implmenented our symbols vs letters rule we'd end up with a higher compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e122fa3-5f80-4c88-872e-b0038d2da7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Ensure the tokenizers directory exists\n",
    "if not os.path.exists('./tokenizers'):\n",
    "    os.makedirs('./tokenizers')\n",
    "\n",
    "# Prepare the tokenizer data to be saved\n",
    "tokenizer_data = {\n",
    "    'stoi': stoi,  # Character to integer mapping\n",
    "    'merges': merges  # Merges dictionary\n",
    "}\n",
    "\n",
    "# Save the tokenizer data using pickle\n",
    "with open(f'./tokenizers/tiny_shakespeare_tokenizer_{vocab_size}.model', 'wb') as f:\n",
    "    pickle.dump(tokenizer_data, f)\n",
    "\n",
    "print(\"Tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14192ed9-ebbf-43a9-9f82-b4b26d89f64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer data using pickle\n",
    "with open(f'./tokenizers/tiny_shakespeare_tokenizer_{vocab_size}.model', 'rb') as f:\n",
    "    loaded_tokenizer_data = pickle.load(f)\n",
    "\n",
    "# Extract the stoi mapping and merges from the loaded data\n",
    "loaded_stoi = loaded_tokenizer_data['stoi']\n",
    "loaded_merges = loaded_tokenizer_data['merges']\n",
    "\n",
    "print(\"Tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "678784cd-f26e-4a83-922d-3a5f99b2fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{(58, 46): 65, (6, 1): 66, (53, 59): 67, (43, 56): 68, (47, 52): 69, (39, 52): 70, (10, 0): 71, (65, 43): 72, (53, 56): 73, (47, 57): 74, (0, 0): 75, (43, 52): 76, (39, 56): 77, (39, 58): 78, (53, 52): 79, (57, 58): 80, (50, 50): 81, (6, 0): 82, (51, 43): 83, (58, 53): 84, (8, 75): 85, (70, 42): 86, (46, 43): 87, (63, 67): 88, (43, 57): 89, (52, 53): 90, (57, 43): 91, (46, 39): 92, (56, 43): 93, (53, 44): 94, (60, 43): 95, (47, 58): 96, (69, 45): 97, (40, 43): 98, (50, 43): 99, (61, 47): 100, (51, 63): 101, (46, 47): 102, (53, 61): 103, (41, 43): 104, (44, 73): 105, (39, 63): 106, (39, 57): 107, (41, 46): 108, (52, 42): 109, (68, 43): 110, (50, 42): 111, (47, 56): 112, (43, 42): 113, (59, 58): 114, (56, 53): 115, (90, 58): 116, (50, 47): 117, (42, 43): 118, (61, 43): 119, (46, 78): 120, (65, 78): 121, (45, 46): 122, (13, 109): 123, (49, 43): 124, (11, 1): 125, (53, 53): 126, (67, 56): 127, (100, 65): 128, (53, 83): 129, (11, 0): 130, (76, 58): 131, (46, 74): 132, (8, 0): 133, (56, 47): 134, (88, 56): 135, (65, 67): 136, (10, 1): 137, (65, 68): 138, (39, 42): 139, (39, 50): 140, (43, 58): 141, (39, 81): 142, (122, 58): 143, (17, 26): 144, (59, 56): 145, (59, 57): 146, (92, 95): 147, (102, 51): 148, (21, 26): 149, (50, 63): 150, (57, 53): 151, (12, 75): 152, (56, 39): 153, (65, 74): 154, (21, 27): 155, (73, 42): 156, (42, 53): 157, (59, 52): 158, (46, 53): 159, (51, 39): 160, (8, 1): 161, (89, 57): 162, (54, 43): 163, (44, 43): 164, (13, 30): 165, (67, 111): 166, (77, 43): 167, (50, 53): 168, (46, 110): 169, (126, 42): 170, (47, 42): 171, (65, 63): 172, (60, 68): 173, (44, 39): 174, (41, 49): 175, (79, 43): 176, (51, 70): 177, (58, 47): 178, (90, 61): 179, (32, 87): 180, (57, 92): 181, (39, 51): 182, (100, 81): 183, (52, 43): 184, (45, 43): 185, (39, 69): 186, (40, 114): 187, (57, 59): 188, (77, 42): 189, (46, 68): 190, (33, 31): 191, (2, 1): 192, (58, 68): 193, (41, 79): 194, (54, 56): 195, (32, 46): 196, (32, 53): 197, (76, 42): 198, (40, 63): 199, (72, 43): 200, (59, 43): 201, (53, 51): 202, (50, 39): 203, (181, 81): 204, (77, 58): 205, (51, 73): 206, (59, 54): 207, (33, 15): 208, (57, 46): 209, (41, 129): 210, (54, 53): 211, (149, 19): 212, (115, 51): 213, (103, 52): 214, (13, 26): 215, (59, 80): 216, (58, 39): 217, (32, 120): 218, (58, 56): 219, (61, 120): 220, (17, 30): 221, (27, 30): 222, (67, 57): 223, (51, 53): 224, (91, 50): 225, (2, 0): 226, (17, 32): 227, (58, 43): 228, (17, 31): 229, (45, 53): 230, (45, 170): 231, (47, 44): 232, (47, 81): 233, (14, 114): 234, (69, 43): 235, (12, 1): 236, (39, 65): 237, (46, 76): 238, (2, 75): 239, (40, 99): 240, (47, 143): 241, (50, 156): 242, (102, 108): 243, (47, 79): 244, (49, 97): 245, (35, 120): 246, (61, 107): 247, (40, 53): 248, (47, 50): 249, (70, 58): 250, (60, 76): 251, (23, 212): 252, (57, 87): 253, (41, 39): 254, (12, 0): 255}\n"
     ]
    }
   ],
   "source": [
    "print(stoi)\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30b8d8fe-2afe-41f3-ac12-0c53360d0e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [22, 33, 24, 21, 227, 71, 27, 1, 30, 202, 43, 53, 66, 30, 202, 43, 53, 192, 61, 87, 93, 105, 43, 1, 205, 1, 136, 1, 30]\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou R\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, merges):\n",
    "        self.stoi = stoi\n",
    "        self.merges = merges\n",
    "        self.itos = {i: s for s, i in stoi.items()}  # Inverse mapping for decoding\n",
    "\n",
    "        self.vocab_len = len(stoi) + len(merges)\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert the text to a list of token IDs, using space for unknown characters\n",
    "        tokens = [self.stoi.get(c, self.stoi[' ']) for c in text]\n",
    "\n",
    "        # Perform merging with the possibility of nested merges\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in self.merges:\n",
    "                # Replace the current pair with its merged token\n",
    "                merged_token = self.merges[pair]\n",
    "                tokens[i] = merged_token\n",
    "                del tokens[i + 1]\n",
    "\n",
    "                # Move back to handle possible nested merges\n",
    "                if i > 0:\n",
    "                    i -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        def expand_token(token):\n",
    "            # Base case: if the token is a direct mapping, return its character\n",
    "            if token in self.itos:\n",
    "                return self.itos[token]\n",
    "            # Recursive case: if the token is a merged token, expand its constituents\n",
    "            elif token in self.merges.values():\n",
    "                pair = next(key for key, value in self.merges.items() if value == token)\n",
    "                return ''.join(expand_token(t) for t in pair)\n",
    "            # Fallback for unknown tokens\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        # Decode each token in the list, handling nested merges recursively\n",
    "        return ''.join(expand_token(token) for token in tokens)\n",
    "\n",
    "# Example usage\n",
    "# Assuming loaded_stoi and loaded_merges are already loaded from the tokenizer.model file\n",
    "\n",
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou R\")\n",
    "print(\"Encoded:\", encoded_text)\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8eb32900-1bcf-461d-9e4d-82ac3c957629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: '\n",
      "'\n",
      "1: ' '\n",
      "2: '!'\n",
      "3: '$'\n",
      "4: '&'\n",
      "5: '''\n",
      "6: ','\n",
      "7: '-'\n",
      "8: '.'\n",
      "9: '3'\n",
      "10: ':'\n",
      "11: ';'\n",
      "12: '?'\n",
      "13: 'A'\n",
      "14: 'B'\n",
      "15: 'C'\n",
      "16: 'D'\n",
      "17: 'E'\n",
      "18: 'F'\n",
      "19: 'G'\n",
      "20: 'H'\n",
      "21: 'I'\n",
      "22: 'J'\n",
      "23: 'K'\n",
      "24: 'L'\n",
      "25: 'M'\n",
      "26: 'N'\n",
      "27: 'O'\n",
      "28: 'P'\n",
      "29: 'Q'\n",
      "30: 'R'\n",
      "31: 'S'\n",
      "32: 'T'\n",
      "33: 'U'\n",
      "34: 'V'\n",
      "35: 'W'\n",
      "36: 'X'\n",
      "37: 'Y'\n",
      "38: 'Z'\n",
      "39: 'a'\n",
      "40: 'b'\n",
      "41: 'c'\n",
      "42: 'd'\n",
      "43: 'e'\n",
      "44: 'f'\n",
      "45: 'g'\n",
      "46: 'h'\n",
      "47: 'i'\n",
      "48: 'j'\n",
      "49: 'k'\n",
      "50: 'l'\n",
      "51: 'm'\n",
      "52: 'n'\n",
      "53: 'o'\n",
      "54: 'p'\n",
      "55: 'q'\n",
      "56: 'r'\n",
      "57: 's'\n",
      "58: 't'\n",
      "59: 'u'\n",
      "60: 'v'\n",
      "61: 'w'\n",
      "62: 'x'\n",
      "63: 'y'\n",
      "64: 'z'\n",
      "65: 'th'\n",
      "66: ', '\n",
      "67: 'ou'\n",
      "68: 'er'\n",
      "69: 'in'\n",
      "70: 'an'\n",
      "71: ':\n",
      "'\n",
      "72: 'the'\n",
      "73: 'or'\n",
      "74: 'is'\n",
      "75: '\n",
      "\n",
      "'\n",
      "76: 'en'\n",
      "77: 'ar'\n",
      "78: 'at'\n",
      "79: 'on'\n",
      "80: 'st'\n",
      "81: 'll'\n",
      "82: ',\n",
      "'\n",
      "83: 'me'\n",
      "84: 'to'\n",
      "85: '.\n",
      "\n",
      "'\n",
      "86: 'and'\n",
      "87: 'he'\n",
      "88: 'you'\n",
      "89: 'es'\n",
      "90: 'no'\n",
      "91: 'se'\n",
      "92: 'ha'\n",
      "93: 're'\n",
      "94: 'of'\n",
      "95: 've'\n",
      "96: 'it'\n",
      "97: 'ing'\n",
      "98: 'be'\n",
      "99: 'le'\n",
      "100: 'wi'\n",
      "101: 'my'\n",
      "102: 'hi'\n",
      "103: 'ow'\n",
      "104: 'ce'\n",
      "105: 'for'\n",
      "106: 'ay'\n",
      "107: 'as'\n",
      "108: 'ch'\n",
      "109: 'nd'\n",
      "110: 'ere'\n",
      "111: 'ld'\n",
      "112: 'ir'\n",
      "113: 'ed'\n",
      "114: 'ut'\n",
      "115: 'ro'\n",
      "116: 'not'\n",
      "117: 'li'\n",
      "118: 'de'\n",
      "119: 'we'\n",
      "120: 'hat'\n",
      "121: 'that'\n",
      "122: 'gh'\n",
      "123: 'And'\n",
      "124: 'ke'\n",
      "125: '; '\n",
      "126: 'oo'\n",
      "127: 'our'\n",
      "128: 'with'\n",
      "129: 'ome'\n",
      "130: ';\n",
      "'\n",
      "131: 'ent'\n",
      "132: 'his'\n",
      "133: '.\n",
      "'\n",
      "134: 'ri'\n",
      "135: 'your'\n",
      "136: 'thou'\n",
      "137: ': '\n",
      "138: 'ther'\n",
      "139: 'ad'\n",
      "140: 'al'\n",
      "141: 'et'\n",
      "142: 'all'\n",
      "143: 'ght'\n",
      "144: 'EN'\n",
      "145: 'ur'\n",
      "146: 'us'\n",
      "147: 'have'\n",
      "148: 'him'\n",
      "149: 'IN'\n",
      "150: 'ly'\n",
      "151: 'so'\n",
      "152: '?\n",
      "\n",
      "'\n",
      "153: 'ra'\n",
      "154: 'this'\n",
      "155: 'IO'\n",
      "156: 'ord'\n",
      "157: 'do'\n",
      "158: 'un'\n",
      "159: 'ho'\n",
      "160: 'ma'\n",
      "161: '. '\n",
      "162: 'ess'\n",
      "163: 'pe'\n",
      "164: 'fe'\n",
      "165: 'AR'\n",
      "166: 'ould'\n",
      "167: 'are'\n",
      "168: 'lo'\n",
      "169: 'here'\n",
      "170: 'ood'\n",
      "171: 'id'\n",
      "172: 'thy'\n",
      "173: 'ver'\n",
      "174: 'fa'\n",
      "175: 'ck'\n",
      "176: 'one'\n",
      "177: 'man'\n",
      "178: 'ti'\n",
      "179: 'now'\n",
      "180: 'The'\n",
      "181: 'sha'\n",
      "182: 'am'\n",
      "183: 'will'\n",
      "184: 'ne'\n",
      "185: 'ge'\n",
      "186: 'ain'\n",
      "187: 'but'\n",
      "188: 'su'\n",
      "189: 'ard'\n",
      "190: 'her'\n",
      "191: 'US'\n",
      "192: '! '\n",
      "193: 'ter'\n",
      "194: 'con'\n",
      "195: 'pr'\n",
      "196: 'Th'\n",
      "197: 'To'\n",
      "198: 'end'\n",
      "199: 'by'\n",
      "200: 'thee'\n",
      "201: 'ue'\n",
      "202: 'om'\n",
      "203: 'la'\n",
      "204: 'shall'\n",
      "205: 'art'\n",
      "206: 'mor'\n",
      "207: 'up'\n",
      "208: 'UC'\n",
      "209: 'sh'\n",
      "210: 'come'\n",
      "211: 'po'\n",
      "212: 'ING'\n",
      "213: 'rom'\n",
      "214: 'own'\n",
      "215: 'AN'\n",
      "216: 'ust'\n",
      "217: 'ta'\n",
      "218: 'That'\n",
      "219: 'tr'\n",
      "220: 'what'\n",
      "221: 'ER'\n",
      "222: 'OR'\n",
      "223: 'ous'\n",
      "224: 'mo'\n",
      "225: 'sel'\n",
      "226: '!\n",
      "'\n",
      "227: 'ET'\n",
      "228: 'te'\n",
      "229: 'ES'\n",
      "230: 'go'\n",
      "231: 'good'\n",
      "232: 'if'\n",
      "233: 'ill'\n",
      "234: 'But'\n",
      "235: 'ine'\n",
      "236: '? '\n",
      "237: 'ath'\n",
      "238: 'hen'\n",
      "239: '!\n",
      "\n",
      "'\n",
      "240: 'ble'\n",
      "241: 'ight'\n",
      "242: 'lord'\n",
      "243: 'hich'\n",
      "244: 'ion'\n",
      "245: 'king'\n",
      "246: 'What'\n",
      "247: 'was'\n",
      "248: 'bo'\n",
      "249: 'il'\n",
      "250: 'ant'\n",
      "251: 'ven'\n",
      "252: 'KING'\n",
      "253: 'she'\n",
      "254: 'ca'\n",
      "255: '?\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "for i in range(256):\n",
    "    print(f\"{i}: '{tokenizer.decode([i])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8a8d9-2485-4ef9-9600-17cbb86b70e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
