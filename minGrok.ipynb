{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMw-8l3kUA5f"
   },
   "source": [
    "# minGrok\n",
    "\n",
    "for the original guide see https://github.com/evintunador/minGrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JOHHIHcjeWzN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./venv/lib/python3.10/site-packages')\n",
    "import dataclasses\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tokenizer import SimpleTokenizer, loaded_stoi, loaded_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oTbAKuix5nt",
    "outputId": "6237f171-09c6-40ac-bf3d-e96a3bdb3928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnx2ACOizX3-",
    "outputId": "fa6c8020-9b55-4702-92b9-f1bd70d6f828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  128\n",
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12]\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "print(\"vocab length: \", tokenizer.vocab_len)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
    "print(\"Encoded:\", encoded_text)\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    # v was defined earlier when we loaded TinyShakespeare. In Grok it's 131,072\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "\n",
    "    # The maximum sequence length that this model might ever be used with.\n",
    "    max_position_embeddings: int = 256 # in Grok it's 8,192\n",
    "\n",
    "    # The number of layers in the model.\n",
    "    num_layers: int = 4 # In Grok it's 64\n",
    "\n",
    "    # The number of attention heads used in the attention layers of the model.\n",
    "    num_attention_heads: int = 4 # In Grok it's 48\n",
    "\n",
    "    # The number of key-value heads for implementing attention.\n",
    "    num_key_value_heads: int = 1 # In Grok it's 8\n",
    "\n",
    "    # The hidden size of the model, AKA the embedding dimension. Each token embedding vector will be this long\n",
    "    hidden_size: int = 96 # In Grok it's 6,144\n",
    "\n",
    "    # How much wider should the inner dimension of the experts be than the model's embedding dimension?\n",
    "    embedding_multiplier_scale: int = 2 # In Grok it's roughly 5.33\n",
    "\n",
    "    # how many experts?\n",
    "    tot_num_experts: int = 4 # in Grok it's 8\n",
    "\n",
    "    # how many active experts per token?\n",
    "    chosen_num_experts: int = 2 # in Grok it's also 2\n",
    "\n",
    "    # what amount of noise should be injected into the router during training?\n",
    "    noise_std = 0.1 # the value for Grok has not been shared\n",
    "\n",
    "    # When we create a loss to encourage all experts to be used, how should that loss be weighted?\n",
    "    lambadada = 10 # Grok's value has not been shared\n",
    "    # excuse my silly naming\n",
    "\n",
    "    # The number of head dimensions\n",
    "    head_dim: int = 24 # In Grok it's 128\n",
    "\n",
    "    # The epsilon used by the rms normalization layers.\n",
    "    rms_norm_eps: float = 1e-5 # this is to promote numerical stability & prevent dividing by 0\n",
    "\n",
    "    # the scaling factor that determines the frequencies for the rotary positional encodings\n",
    "    rope_theta = 100.0 # Grok and most models use 10,000\n",
    "    # smaller models should use a smaller theta, but I'm just guessing here. 1000 might work too\n",
    "\n",
    "    # whether to use a linear layer after normalization\n",
    "    use_scale: bool = True # same in Grok\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # the dropout rate to use during training\n",
    "    dropout = 0.05\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "\n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQA(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Query Attention which supports a distinct number of attention heads for queries and key-values (KV).\n",
    "    In the case where the same number of queries and key-values are used, this implemenation is equivalent to regular Multi-Head Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        assert self.num_heads % self.num_kv_heads == 0\n",
    "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.head_dim = config.head_dim\n",
    "        self.theta = config.rope_theta\n",
    "\n",
    "        self.q_size = self.num_heads * self.head_dim\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.hidden_size, (self.num_heads + 2 * self.num_kv_heads) * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "\n",
    "        # Create a mask tensor with shape [batch_size, num_heads, seq_len, seq_len]\n",
    "        self.mask = torch.tril(torch.ones((config.max_position_embeddings, config.max_position_embeddings), \n",
    "                                     dtype=torch.uint8)).view(1, 1, config.max_position_embeddings, config.max_position_embeddings).to(dtype=torch.bool)\n",
    "        #self.mask = mask.expand(-1, self.num_heads, -1, -1)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states_shape = hidden_states.shape\n",
    "        assert len(hidden_states_shape) == 3\n",
    "        batch_size, input_len, _ = hidden_states_shape\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([self.q_size, self.kv_size, self.kv_size],dim=-1)\n",
    "\n",
    "        # Reshapes each to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = apply_rotary_emb(xq, self.head_dim, self.theta)\n",
    "        xk = apply_rotary_emb(xk, self.head_dim, self.theta)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
    "            xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "\n",
    "        # Transposes to align them for the batch matrix multiplication in attention calculation.\n",
    "        q = xq.transpose(1, 2)\n",
    "        k = xk.transpose(1, 2)\n",
    "        v = xv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention logits by performing a batch matrix multiplication between queries and keys\n",
    "        logits = torch.matmul(q, k.transpose(2, 3))\n",
    "\n",
    "        # Grok's unusual scaling method\n",
    "        # If anyone knows why they use 0.08838834764831845 in Grok please lmk. Maybe it's a learned value?\n",
    "        logits *= 0.08838834764831845\n",
    "        # Next here we'll scale and clip our attention logits\n",
    "        # the tanh is a nonlinear function that pushes all of the entries in logits into the range (-1, 1)\n",
    "        # then they're scaled up to the range (-30, 30). The number 30 is an arbitrary choice\n",
    "        # the purpose of this scaling is to regularize and prevent numerical stability that might otherwise mess with the upcoming softmax\n",
    "        max_attn_val = torch.tensor(30.0, dtype = logits.dtype)\n",
    "        logits = max_attn_val * torch.tanh(logits / max_attn_val)\n",
    "        # other transformers would replace the last three lines with a multiplication by torch.sqrt(self.hidden_size)\n",
    "\n",
    "        # Applies the lower-triangular mask to the attention logits\n",
    "        logits = torch.where(self.mask[..., :input_len, :input_len].expand_as(logits), logits, torch.tensor(-1e30, device=logits.device, dtype=logits.dtype))\n",
    "\n",
    "        # Applies softmax to the logits to obtain attention probabilities\n",
    "        scores = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        output = torch.matmul(scores, v)\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)\n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, model_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(model_dim, hidden_dim * 2, bias=False)  # Double the output for gating\n",
    "        self.layer2 = nn.Linear(hidden_dim, model_dim, bias=False)  # Output layer remains the same\n",
    "\n",
    "    def forward(self, x):\n",
    "      # Split the output of the first layer for gating\n",
    "        x, gate = self.layer1(x).chunk(2, dim=-1)\n",
    "\n",
    "        # Apply GeLU to the gate, and then multiply element-wise\n",
    "        x = F.gelu(gate) * x\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    def __init__(self, input_size, tot_num_experts, noise_std: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.tot_num_experts = tot_num_experts\n",
    "        self.router_weights = nn.Linear(input_size, tot_num_experts, bias=False)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def forward(self, inputs, training: bool = False):\n",
    "        routing_logits = self.router_weights(inputs)\n",
    "        if training: routing_logits = routing_logits + torch.randn_like(routing_logits) * self.noise_std\n",
    "        routing_probs = F.softmax(routing_logits, dim=-1)\n",
    "        return routing_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, model_dim, expert_hidden_dim, tot_num_experts, chosen_num_experts, noise_std):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.tot_num_experts = tot_num_experts\n",
    "        self.chosen_num_experts = chosen_num_experts\n",
    "        self.experts = nn.ModuleList([Expert(model_dim, expert_hidden_dim) for _ in range(tot_num_experts)])\n",
    "        self.router = Router(model_dim, tot_num_experts, noise_std)\n",
    "\n",
    "    def forward(self, inputs, training: bool = False):\n",
    "        b, seq_len, _ = inputs.shape\n",
    "\n",
    "        # get the output of all the experts\n",
    "        expert_outputs = [expert(inputs.view(-1, self.model_dim)) for expert in self.experts]\n",
    "        expert_outputs = torch.cat(expert_outputs, dim=0).view(b, seq_len, self.tot_num_experts, self.model_dim)\n",
    "\n",
    "        # get the output of the router and create out expert mask\n",
    "        routing_probs = F.softmax(self.router(inputs), dim=-1)\n",
    "        with torch.no_grad():\n",
    "          expert_indices = torch.topk(routing_probs, k=self.chosen_num_experts, sorted=True).indices\n",
    "          multi_hot_indices = torch.zeros(b, seq_len, self.tot_num_experts, device=inputs.device)\n",
    "          multi_hot_indices = multi_hot_indices.scatter(2, expert_indices, 1)\n",
    "\n",
    "        # Apply the multi-hot mask (first expand dimensions for broadcasting)\n",
    "        multi_hot_expanded = multi_hot_indices.unsqueeze(-1).expand_as(expert_outputs)\n",
    "        output_masked = expert_outputs * multi_hot_expanded.float()\n",
    "\n",
    "        # then weight our experts' outputs by the softmax values (which we first must broadcast to the right shape) and sum them\n",
    "        routing_probs_expanded = routing_probs.unsqueeze(-1).expand_as(output_masked)\n",
    "        MoE_output = (output_masked * routing_probs_expanded).sum(dim=2)\n",
    "\n",
    "        return MoE_output, routing_probs # we also output routing_probs to be used in the loss function later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module): # the same RMSNorm we wrote earlier\n",
    "    def __init__(self, num_features, eps=1e-5, use_scale=True):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(num_features)) if use_scale else None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the mean squared value for each feature\n",
    "        mean_squared = inputs.pow(2).mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # Normalize inputs\n",
    "        normed_inputs = inputs * torch.rsqrt(mean_squared + self.eps)\n",
    "\n",
    "        # Apply scale if it exists\n",
    "        if self.scale is not None:\n",
    "            normed_inputs = normed_inputs * self.scale\n",
    "\n",
    "        return normed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder layer that integrates the Attention mechanism and MoE. It includes\n",
    "    normalization steps both before and after the MQA and MoE but never actually normalized the residual connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mqa = MQA(config)\n",
    "\n",
    "        self.moe = MoELayer(\n",
    "            model_dim = config.hidden_size,\n",
    "            expert_hidden_dim = config.hidden_size * config.embedding_multiplier_scale,\n",
    "            tot_num_experts = config.tot_num_experts,\n",
    "            chosen_num_experts = config.chosen_num_experts,\n",
    "            noise_std = config.noise_std\n",
    "        )\n",
    "\n",
    "        self.pre_mqa_norm = RMSNorm(config.hidden_size, eps = config.rms_norm_eps, use_scale = config.use_scale)\n",
    "        self.post_mqa_norm = RMSNorm(config.hidden_size, eps = config.rms_norm_eps, use_scale = config.use_scale)\n",
    "        self.pre_moe_norm = RMSNorm(config.hidden_size, eps = config.rms_norm_eps, use_scale = config.use_scale)\n",
    "        self.post_moe_norm = RMSNorm(config.hidden_size, eps = config.rms_norm_eps, use_scale = config.use_scale)\n",
    "\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, training: bool = False) -> torch.Tensor:\n",
    "        if training:\n",
    "            x = x + self.drop(self.post_mqa_norm(self.mqa(self.pre_mqa_norm(x))))\n",
    "            moe_out, routing_probs = self.moe(self.pre_moe_norm(x), training)\n",
    "            x = x + self.drop(self.post_moe_norm(moe_out))\n",
    "        else:\n",
    "            x = x + self.post_mqa_norm(self.mqa(self.pre_mqa_norm(x)))\n",
    "            moe_out, routing_probs = self.moe(self.pre_moe_norm(x), training)\n",
    "            x = x + self.post_moe_norm(moe_out)\n",
    "        return x, routing_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class minGrok(nn.Module):\n",
    "\n",
    "    def __init__(self, config, tokenizer):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # the attention heads need to cleanly divide up the hidden_size of the model so that we can split it all apart & combine back together\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "\n",
    "        self.max_seq_len = config.max_position_embeddings\n",
    "        self.head_dim = config.head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "         # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(self.vocab_size, config.hidden_size)\n",
    "\n",
    "        # Initialize a sequence of DecoderLayer instances as specified by the number of layers in the config\n",
    "        self.layers = nn.ModuleList(DecoderLayer(config) for _ in range(config.num_layers))\n",
    "\n",
    "        # Initialize a normalization layer to be applied after the last decoder layer, stabilizing the output\n",
    "        self.final_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        # the primary loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "        # the hyperparameter weighting the secondary loss function\n",
    "        self.lambadada = config.lambadada\n",
    "\n",
    "    def calc_moe_loss(self, routing_probs_list):\n",
    "        # this is silly and inefficient but i'm tired and bored of this project ngl\n",
    "        # basically i'm choosing to sum the per-layer MoE variances\n",
    "        cum_var = torch.tensor(0.0) # this will be encouraged to be 0 so it doesn't even matter if we record the gradient\n",
    "        for routing_probs in routing_probs_list:\n",
    "            expert_usage = routing_probs.sum(dim=0)\n",
    "            usage_mean = expert_usage.mean()\n",
    "            expert_variance = ((expert_usage - usage_mean) ** 2).mean()\n",
    "            cum_var = cum_var + expert_variance\n",
    "\n",
    "        return cum_var\n",
    "\n",
    "    # a more efficient version ChatGPT4 made that i'm too lazy to test, but go ahead if you want\n",
    "    #def calc_moe_loss(self, routing_probs_list):\n",
    "        # Concatenate all tensors along a new dimension (say, dim=0)\n",
    "        # This results in a new tensor of shape (N, b, t, c) where N is the number of tensors in routing_probs_list\n",
    "        #all_routing_probs = torch.cat([x.unsqueeze(0) for x in routing_probs_list], dim=0)\n",
    "        \n",
    "        # Sum across the batch (b) and time (t) dimensions, resulting in a shape of (N, c)\n",
    "        #expert_usage = all_routing_probs.sum(dim=1).sum(dim=1)\n",
    "        \n",
    "        # Calculate the mean across the new dimension (N) and the experts (c), resulting in a single mean value\n",
    "        #usage_mean = expert_usage.mean(dim=0).mean(dim=0)\n",
    "        \n",
    "        # Calculate the variance\n",
    "        #expert_variance = ((expert_usage - usage_mean) ** 2).mean(dim=0).mean(dim=0)\n",
    "        \n",
    "        # Sum the variance across all layers (N)\n",
    "        #cum_var = expert_variance.sum()\n",
    "        \n",
    "        #return cum_var\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len) list of integer token ids\n",
    "        target_token_ids: torch.Tensor = None, # a shape (batch_size, input_seq_len) list of token ids to train on\n",
    "        ) -> torch.Tensor:\n",
    "        training = False if target_token_ids is None else True\n",
    "\n",
    "        # turn the input tokens into the first resudial state using the embedding matrix\n",
    "        x = self.embedder(input_token_ids) * self.config.hidden_size**0.5 # Grok normalizes the embedding by sqrt(hidden_size)\n",
    "\n",
    "        # initialize a list to store the routing probs of each layer in\n",
    "        routing_probs_list = []\n",
    "        # Iteratively process the input through each DecoderLayer\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            x, routing_probs = layer(x, training)\n",
    "            if training: routing_probs_list.append(routing_probs)\n",
    "\n",
    "        # Apply normalization to the output of the final decoder layer\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # grabbing the weights of the embedding matrix shape (vocab_size, hidden_dim) for use as the output layer\n",
    "        embedder_weight = self.embedder.weight\n",
    "\n",
    "        # the embedding matrix is also used as the output layer\n",
    "        # this saves on parameters & makes sense for interpretability\n",
    "        # (batch_size, input_len, hidden_size) @ (hidden_size, vocab_size) -> (batch_size, input_len, vocab_size)\n",
    "        logits = torch.matmul(x, embedder_weight.t())\n",
    "\n",
    "        if training: # if we are training\n",
    "            batch_size, input_len, vocab_size = logits.shape\n",
    "\n",
    "            # we reshape our logits & targets before calculating cross-entropy loss\n",
    "            CEloss = self.criterion(logits.view(batch_size*input_len, vocab_size),\n",
    "                                    target_token_ids.view(batch_size*input_len))\n",
    "            \n",
    "            # calculating the MoE loss that encourages all experts to be utilized\n",
    "            MoEloss = self.calc_moe_loss(routing_probs_list)\n",
    "\n",
    "            # our final loss value\n",
    "            loss = CEloss + MoEloss * self.lambadada\n",
    "        else:\n",
    "            loss = None # if we're not training, then we don't need to calculate loss\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions from Grok's output.\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling\n",
    "        \"\"\"\n",
    "        # Select the last element for each sequence.\n",
    "        logits = logits[:,-1,:]\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        logits.div_(temperature) # div_ is an in-place operation which is ok since we don't record gradients during inference\n",
    "\n",
    "        # Calculate probabilities\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "\n",
    "        # calculating top_k\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort)  # the original probabilities with excluded tokens changed to 0.0\n",
    "\n",
    "        # calculating top_k\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1) # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        top_ks_mask = top_ks_mask >= top_k # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True)) # Re-normalization so that total probabilities add up to 1\n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort,\n",
    "                             dim=-1,\n",
    "                             index=torch.argsort(probs_idx, dim=-1))\n",
    "\n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        return next_token_id\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_len: int = 100, # the model will output 100 tokens\n",
    "        temperature: float = 0.95, # 0.95 is pretty close to not even using temperature at all (1.0 would be no effect)\n",
    "        top_p: float = 1.0, # defaulting to 1 means we essentially don't use top-p\n",
    "        top_k: int = 65, # setting top_k = vocab_size means we're effectively not using top_k at all\n",
    "    ) -> str:\n",
    "        \"\"\"Generates responses for given prompts using Grok model.\"\"\"\n",
    "\n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=self.config.device).unsqueeze(0)\n",
    "\n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        assert len(tokens) + output_len <= self.config.max_position_embeddings\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len])\n",
    "\n",
    "            next_token = self.Sampler(\n",
    "                logits = logits, # the actual output of the model\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                top_k = top_k\n",
    "            )\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I5hAq2VcvpdS"
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F6M2WCXs_Vld"
   },
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oaiM9_Od_Vnv"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 10): # to periodically estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pdPqc-L_VqU",
    "outputId": "fea4a729-a9d5-437c-90fb-663e70ab03f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992.352 K parameters\n",
      "minGrok(\n",
      "  (embedder): Embedding(128, 96)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x DecoderLayer(\n",
      "      (mqa): MQA(\n",
      "        (qkv_proj): Linear(in_features=96, out_features=144, bias=False)\n",
      "        (o_proj): Linear(in_features=96, out_features=96, bias=False)\n",
      "      )\n",
      "      (moe): MoELayer(\n",
      "        (experts): ModuleList(\n",
      "          (0-3): 4 x Expert(\n",
      "            (layer1): Linear(in_features=96, out_features=384, bias=False)\n",
      "            (layer2): Linear(in_features=192, out_features=96, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (router): Router(\n",
      "          (router_weights): Linear(in_features=96, out_features=4, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (pre_mqa_norm): RMSNorm()\n",
      "      (post_mqa_norm): RMSNorm()\n",
      "      (pre_moe_norm): RMSNorm()\n",
      "      (post_moe_norm): RMSNorm()\n",
      "      (drop): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): RMSNorm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate a new model\n",
    "model = minGrok(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9KhAPH5g_VsX"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 5000\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 100\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 32\n",
    "\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OECt3NLpBGKc",
    "outputId": "59d3397d-4a49-41be-c2eb-f3804dad0533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4603, val loss 2.6593, time elapsed: 1.15 seconds\n",
      "step 100: train loss 2.4402, val loss 2.6460, time elapsed: 102.51 seconds\n",
      "step 200: train loss 2.4155, val loss 2.6502, time elapsed: 201.02 seconds\n",
      "step 300: train loss 2.4238, val loss 2.6558, time elapsed: 302.31 seconds\n",
      "step 400: train loss 2.4197, val loss 2.6455, time elapsed: 399.47 seconds\n",
      "step 500: train loss 2.4427, val loss 2.6603, time elapsed: 496.25 seconds\n",
      "step 600: train loss 2.4317, val loss 2.6517, time elapsed: 594.00 seconds\n",
      "step 700: train loss 2.4523, val loss 2.6506, time elapsed: 695.84 seconds\n",
      "step 800: train loss 2.4124, val loss 2.6615, time elapsed: 799.43 seconds\n",
      "step 900: train loss 2.4369, val loss 2.6301, time elapsed: 900.91 seconds\n",
      "step 1000: train loss 2.4354, val loss 2.6568, time elapsed: 1002.66 seconds\n",
      "step 1100: train loss 2.4033, val loss 2.6486, time elapsed: 1098.36 seconds\n",
      "step 1200: train loss 2.4403, val loss 2.6508, time elapsed: 1194.98 seconds\n",
      "step 1300: train loss 2.4188, val loss 2.6256, time elapsed: 1291.53 seconds\n",
      "step 1400: train loss 2.4036, val loss 2.6346, time elapsed: 1388.99 seconds\n",
      "step 1500: train loss 2.4113, val loss 2.6279, time elapsed: 1485.93 seconds\n",
      "step 1600: train loss 2.4166, val loss 2.6373, time elapsed: 1582.42 seconds\n",
      "step 1700: train loss 2.4311, val loss 2.6474, time elapsed: 1678.93 seconds\n",
      "step 1800: train loss 2.4233, val loss 2.6146, time elapsed: 1775.62 seconds\n",
      "step 1900: train loss 2.4098, val loss 2.6181, time elapsed: 1872.80 seconds\n",
      "step 1999: train loss 2.4265, val loss 2.6327, time elapsed: 1978.33 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "\n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Create a shorter, more concise filename\n",
    "filename = (f'{model.__class__.__name__}'\n",
    "           f'-v{config.vocab_size}'\n",
    "           f'-max_t{config.max_position_embeddings}'\n",
    "           f'-layers{config.num_layers}'\n",
    "           f'-heads{config.num_attention_heads}'\n",
    "           f'-kv_heads{config.num_key_value_heads}'\n",
    "           f'-hidden{config.hidden_size}'\n",
    "           f'-embedding_multiplier_scale{config.embedding_multiplier_scale}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-theta{config.rope_theta}'\n",
    "           f'-lr{learning_rate}'\n",
    "           f'-decay{weight_decay}'\n",
    "            f'-tot_num_experts{config.tot_num_experts}'\n",
    "            f'-chosen_num_experts{config.chosen_num_experts}'\n",
    "            f'-use_scale{config.use_scale}'\n",
    "           f'-batch{batch_size}'\n",
    "            f'-train_iter{max_iters}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d_%H-%M-%S\")}.pth')\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(model_dir, filename)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3z_2iYbvqUV"
   },
   "source": [
    "### Alternatively, you can load the 1m parameter model I already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "B2exYhJGvxDt",
    "outputId": "b23129f9-b5b8-42c6-fd20-9be5857b859f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992.352 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "minGrok(\n",
       "  (embedder): Embedding(128, 96)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x DecoderLayer(\n",
       "      (mqa): MQA(\n",
       "        (qkv_proj): Linear(in_features=96, out_features=144, bias=False)\n",
       "        (o_proj): Linear(in_features=96, out_features=96, bias=False)\n",
       "      )\n",
       "      (moe): MoELayer(\n",
       "        (experts): ModuleList(\n",
       "          (0-3): 4 x Expert(\n",
       "            (layer1): Linear(in_features=96, out_features=384, bias=False)\n",
       "            (layer2): Linear(in_features=192, out_features=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (router): Router(\n",
       "          (router_weights): Linear(in_features=96, out_features=4, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (pre_mqa_norm): RMSNorm()\n",
       "      (post_mqa_norm): RMSNorm()\n",
       "      (pre_moe_norm): RMSNorm()\n",
       "      (post_moe_norm): RMSNorm()\n",
       "      (drop): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = minGrok(config, tokenizer).to(config.device)\n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/minGrok-v128-max_t256-layers4-heads4-kv_heads1-hidden96-embedding_multiplier_scale2-head_dim24-theta100.0-lr0.0003-decay0.01-tot_num_experts4-chosen_num_experts2-use_scaleTrue-batch32-train_iter5000--2024-03-21_18-20-32.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFa4Pfi2vx3e"
   },
   "source": [
    "### Testing (performing inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rK5bkaFmv1dH",
    "outputId": "34c281b7-8870-4292-8be9-abb90af6688c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou in.\n",
      "\n",
      "Tome?\n",
      "\n",
      "Nurse:\n",
      "Third peaguisrener:\n",
      "Lo, show and go yours, here mace meraticome\n",
      "For a thee be oneeget and the lambron a it-ntard; whileTHerle you fair murfeen a 'tis to like.\n",
      "\n",
      "MENEnguill Yort death their honour mind,\n",
      "If such therese the curry woront, I that mine,\n",
      "Why the stays is of him in still.\n",
      "\n",
      "F\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou\" # the classic line\n",
    "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hikwp10DQQEb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
